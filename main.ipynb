{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0098b6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 2610.674\n",
      "Epoch 10, loss: 2463.013\n",
      "Epoch 20, loss: 2324.259\n",
      "Epoch 30, loss: 2193.678\n",
      "Epoch 40, loss: 2068.025\n",
      "Epoch 50, loss: 1945.178\n",
      "Epoch 60, loss: 1824.658\n",
      "Epoch 70, loss: 1708.096\n",
      "Epoch 80, loss: 1602.005\n",
      "Epoch 90, loss: 1515.582\n",
      "Epoch 100, loss: 1440.092\n",
      "Epoch 110, loss: 1373.528\n",
      "Epoch 120, loss: 1313.496\n",
      "Epoch 130, loss: 1259.835\n",
      "Epoch 140, loss: 1214.785\n",
      "Epoch 150, loss: 1174.679\n",
      "Epoch 160, loss: 1137.427\n",
      "Epoch 170, loss: 1103.070\n",
      "Epoch 180, loss: 1072.207\n",
      "Epoch 190, loss: 1045.363\n",
      "Epoch 200, loss: 1020.172\n",
      "Epoch 210, loss: 996.919\n",
      "Epoch 220, loss: 975.463\n",
      "Epoch 230, loss: 955.838\n",
      "Epoch 240, loss: 937.442\n",
      "Epoch 250, loss: 920.432\n",
      "Epoch 260, loss: 904.694\n",
      "Epoch 270, loss: 891.453\n",
      "Epoch 280, loss: 880.922\n",
      "Epoch 290, loss: 871.202\n",
      "Epoch 300, loss: 862.494\n",
      "Epoch 310, loss: 854.123\n",
      "Epoch 320, loss: 845.112\n",
      "Epoch 330, loss: 838.060\n",
      "Epoch 340, loss: 830.849\n",
      "Epoch 350, loss: 824.230\n",
      "Epoch 360, loss: 817.352\n",
      "Epoch 370, loss: 811.208\n",
      "Epoch 380, loss: 804.420\n",
      "Epoch 390, loss: 798.591\n",
      "Epoch 400, loss: 793.208\n",
      "Epoch 410, loss: 789.604\n",
      "Epoch 420, loss: 786.796\n",
      "Epoch 430, loss: 784.166\n",
      "Epoch 440, loss: 782.472\n",
      "Epoch 450, loss: 780.409\n",
      "Epoch 460, loss: 779.714\n",
      "Epoch 470, loss: 779.091\n",
      "Epoch 480, loss: 779.038\n",
      "Epoch 490, loss: 778.783\n",
      "Epoch 500, loss: 778.666\n",
      "Epoch 510, loss: 778.258\n",
      "Epoch 520, loss: 778.203\n",
      "Epoch 530, loss: 778.022\n",
      "Epoch 540, loss: 777.812\n",
      "Epoch 550, loss: 777.643\n",
      "Epoch 560, loss: 777.757\n",
      "Epoch 570, loss: 777.567\n",
      "Epoch 580, loss: 777.464\n",
      "Epoch 590, loss: 777.376\n",
      "Epoch 600, loss: 777.209\n",
      "Epoch 610, loss: 777.264\n",
      "Epoch 620, loss: 777.131\n",
      "Epoch 630, loss: 776.921\n",
      "Epoch 640, loss: 776.831\n",
      "Epoch 650, loss: 776.626\n",
      "Epoch 660, loss: 776.697\n",
      "Epoch 670, loss: 776.506\n",
      "Epoch 680, loss: 776.316\n",
      "Epoch 690, loss: 776.155\n",
      "Epoch 700, loss: 775.986\n",
      "Epoch 710, loss: 776.094\n",
      "Epoch 720, loss: 776.088\n",
      "Epoch 730, loss: 776.072\n",
      "Epoch 740, loss: 776.081\n",
      "Epoch 750, loss: 775.985\n",
      "Epoch 760, loss: 776.020\n",
      "Epoch 770, loss: 775.932\n",
      "Epoch 780, loss: 775.822\n",
      "Epoch 790, loss: 775.907\n",
      "Epoch 800, loss: 775.585\n",
      "Epoch 810, loss: 775.844\n",
      "Epoch 820, loss: 775.573\n",
      "Epoch 830, loss: 775.876\n",
      "Epoch 840, loss: 775.755\n",
      "Epoch 850, loss: 775.644\n",
      "Epoch 860, loss: 775.643\n",
      "Epoch 870, loss: 775.723\n",
      "Epoch 880, loss: 775.327\n",
      "Epoch 890, loss: 775.380\n",
      "Epoch 900, loss: 775.759\n",
      "Epoch 910, loss: 775.390\n",
      "Epoch 920, loss: 775.467\n",
      "Epoch 930, loss: 775.539\n",
      "Epoch 940, loss: 775.264\n",
      "Epoch 950, loss: 775.512\n",
      "Epoch 960, loss: 775.426\n",
      "Epoch 970, loss: 775.456\n",
      "Epoch 980, loss: 775.413\n",
      "Epoch 990, loss: 775.476\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.linalg import vector_norm\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "\n",
    "from distances import *\n",
    "from methods import * \n",
    "from optimizers import * \n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "lr = 0.01\n",
    "num_epochs = 1000\n",
    "normalize = True\n",
    "\n",
    "geodesic = True\n",
    "min_dist = 1.\n",
    "\n",
    "data = torch.randn(50, 50)\n",
    "data_dist_matrix = dist_matrix(data, Euclidean)\n",
    "\n",
    "#IsoMap-style geodesic distance for data\n",
    "if geodesic:\n",
    "#     truncated_matrix = torch.where(data_dist_matrix < min_dist, data_dist_matrix, torch.inf)\n",
    "#     data_dist_matrix = dijkstra(truncated_matrix.detach().cpu().numpy())\n",
    "#     data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "#     data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix), data_dist_matrix)\n",
    "    \n",
    "    data_nn_matrix = kneighbors_graph(data, 3, mode='distance', include_self=False)\n",
    "    data_nn_matrix = data_nn_matrix.toarray()\n",
    "    data_dist_matrix = dijkstra(data_nn_matrix)\n",
    "    data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "    data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix), data_dist_matrix)\n",
    "       \n",
    "    \n",
    "\n",
    "# torch.manual_seed(42)       \n",
    "# torch.cuda.empty_cache()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Using device: ', device)\n",
    "\n",
    "\n",
    "\n",
    "#model = MDS(data.shape[0], latent_dim, Poincare)\n",
    "#model = Isomap(data.shape[0], latent_dim, Poincare)\n",
    "\n",
    "data_binary_matrix = (data_nn_matrix > 0.).astype(int)\n",
    "model = Contrastive(data.shape[0], latent_dim, Poincare)\n",
    "\n",
    "\n",
    "\n",
    "#optimizer = StandardOptim(model, lr=lr)\n",
    "optimizer = PoincareOptim(model, lr=lr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(num_epochs):\n",
    "        if normalize:\n",
    "            model.normalize()\n",
    "\n",
    "        # print('norms', vector_norm(model.embeddings, dim=-1).mean().item(), vector_norm(model.embeddings, dim=-1).max().item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        #loss = model.loss_fun(data_dist_matrix)\n",
    "        loss = model.loss_fun(data_binary_matrix, temperature=1)\n",
    "\n",
    "        loss.backward()\n",
    "        # print('grads', vector_norm(model.embeddings.grad, dim=-1).mean().item())\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, loss: {loss:.3f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab90113a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dabfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from constants import *\n",
    "from methods import *\n",
    "from optimizers import * \n",
    "from utils.visulization import *\n",
    "from OdorDataset import OdorMonoDataset\n",
    "from utils.helpers import *\n",
    "latent_dim = 2\n",
    "lr = 0.1\n",
    "num_epochs = 100000\n",
    "normalize = False\n",
    "geodesic = False\n",
    "min_dist = 1.\n",
    "\n",
    "\n",
    "\n",
    "dataset_name='gslf'\n",
    "model_name = 'molformer'\n",
    "batch_size =10\n",
    "\n",
    "def set_seeds(seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "def select_descriptors(dataset_name):\n",
    "    if dataset_name=='sagar':\n",
    "        return sagar_descriptors\n",
    "    elif dataset_name=='keller':\n",
    "        return keller_descriptors\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# set_seeds(2025)\n",
    "base_dir = '../../../T5 EVO/alignment_olfaction_datasets/curated_datasets/'\n",
    "input_embeddings = f'embeddings/{model_name}/{dataset_name}_{model_name}_embeddings_13_Apr17.csv'\n",
    "\n",
    "dataset = OdorMonoDataset(base_dir, input_embeddings, transform=None, grand_avg=False, descriptors=select_descriptors(dataset_name))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "    # del dataset\n",
    "\n",
    "\n",
    "def geo_distance(data):\n",
    "    #     truncated_matrix = torch.where(data_dist_matrix < min_dist, data_dist_matrix, torch.inf)\n",
    "    #     data_dist_matrix = dijkstra(truncated_matrix.detach().cpu().numpy())\n",
    "    #     data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "    #     data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix), data_dist_matrix)\n",
    "\n",
    "    data_nn_matrix = kneighbors_graph(data, 3, mode='distance', include_self=False)\n",
    "    data_nn_matrix = data_nn_matrix.toarray()\n",
    "    data_dist_matrix = dijkstra(data_nn_matrix)\n",
    "    data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "    data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix),data_dist_matrix)\n",
    "    return data_dist_matrix\n",
    "\n",
    "\n",
    "#IsoMap-style geodesic distance for data\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.empty_cache()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Using device: ', device)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#model = MDS(data.shape[0], latent_dim, Poincare)\n",
    "# print(len(dataset))\n",
    "model = Isomap(len(dataset), latent_dim, Euclidean)\n",
    "\n",
    "#optimizer = StandardOptim(model, lr=lr)\n",
    "optimizer = StandardOptim(model, lr=lr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(num_epochs):\n",
    "        total_loss=0\n",
    "        if normalize:\n",
    "            model.normalize()\n",
    "        for idx, batch in data_loader:\n",
    "\n",
    "            if geodesic:\n",
    "                data_dist_matrix = geo_distance(batch)\n",
    "            else:\n",
    "                data_dist_matrix = dist_matrix(batch, Euclidean)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss_fun(data_dist_matrix,idx)\n",
    "            loss.backward()\n",
    "            optimizer.step(idx)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # print('norms', vector_norm(model.embeddings, dim=-1).mean().item(), vector_norm(model.embeddings, dim=-1).max().item())\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, loss: {total_loss/len(data_loader):.3f}')\n",
    "\n",
    "    scatterplot_2d(model.embeddings.detach().cpu().numpy(), labels=None, title='Poincare Embeddings')\n",
    "\n",
    "\n",
    "\n",
    "    size1 = (0.3, 0.28)\n",
    "    size2 = (0.6, 1)\n",
    "    size3 = (1, 0.35)\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 35\n",
    "    df_gslf_mols = prepare_goodscentleffignwell_mols(base_dir)\n",
    "    pom_frame(np.asarray(model.embeddings.detach().cpu().numpy().values.tolist()),\n",
    "              np.asarray(df_gslf_mols.y.values.tolist()), \"/kaggle/working/\", gs_lf_tasks, \"molformer\", size1, size2,\n",
    "              size3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea26ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 289.714\n",
      "Epoch 10, loss: 83.460\n",
      "Epoch 20, loss: 48.773\n",
      "Epoch 30, loss: 49.475\n",
      "Epoch 40, loss: 137.450\n",
      "Epoch 50, loss: 40.684\n",
      "Epoch 60, loss: 95.005\n",
      "Epoch 70, loss: 72.771\n",
      "Epoch 80, loss: 106.791\n",
      "Epoch 90, loss: 103.854\n",
      "Epoch 100, loss: 107.954\n",
      "Epoch 110, loss: 118.721\n",
      "Epoch 120, loss: 76.412\n",
      "Epoch 130, loss: 49.903\n",
      "Epoch 140, loss: 126.539\n",
      "Epoch 150, loss: 70.090\n",
      "Epoch 160, loss: 37.535\n",
      "Epoch 170, loss: 119.115\n",
      "Epoch 180, loss: 70.411\n",
      "Epoch 190, loss: 41.788\n",
      "Epoch 200, loss: 68.861\n",
      "Epoch 210, loss: 67.636\n",
      "Epoch 220, loss: 32.539\n",
      "Epoch 230, loss: 65.456\n",
      "Epoch 240, loss: 106.144\n",
      "Epoch 250, loss: 30.989\n",
      "Epoch 260, loss: 37.801\n",
      "Epoch 270, loss: 61.228\n",
      "Epoch 280, loss: 60.713\n",
      "Epoch 290, loss: 29.083\n",
      "Epoch 300, loss: 73.192\n",
      "Epoch 310, loss: 94.391\n",
      "Epoch 320, loss: 28.307\n",
      "Epoch 330, loss: 27.407\n",
      "Epoch 340, loss: 55.901\n",
      "Epoch 350, loss: 33.678\n",
      "Epoch 360, loss: 53.649\n",
      "Epoch 370, loss: 26.198\n",
      "Epoch 380, loss: 84.631\n",
      "Epoch 390, loss: 50.512\n",
      "Epoch 400, loss: 80.305\n",
      "Epoch 410, loss: 36.232\n",
      "Epoch 420, loss: 47.931\n",
      "Epoch 430, loss: 47.225\n",
      "Epoch 440, loss: 23.447\n",
      "Epoch 450, loss: 46.028\n",
      "Epoch 460, loss: 28.508\n",
      "Epoch 470, loss: 54.771\n",
      "Epoch 480, loss: 22.269\n",
      "Epoch 490, loss: 31.331\n",
      "Epoch 500, loss: 41.591\n",
      "Epoch 510, loss: 30.713\n",
      "Epoch 520, loss: 39.071\n",
      "Epoch 530, loss: 25.105\n",
      "Epoch 540, loss: 20.399\n",
      "Epoch 550, loss: 45.604\n",
      "Epoch 560, loss: 35.888\n",
      "Epoch 570, loss: 18.720\n",
      "Epoch 580, loss: 26.843\n",
      "Epoch 590, loss: 25.815\n",
      "Epoch 600, loss: 17.862\n",
      "Epoch 610, loss: 38.538\n",
      "Epoch 620, loss: 17.111\n",
      "Epoch 630, loss: 30.978\n",
      "Epoch 640, loss: 29.797\n",
      "Epoch 650, loss: 15.971\n",
      "Epoch 660, loss: 15.132\n",
      "Epoch 670, loss: 15.231\n",
      "Epoch 680, loss: 14.264\n",
      "Epoch 690, loss: 25.146\n",
      "Epoch 700, loss: 17.039\n",
      "Epoch 710, loss: 24.337\n",
      "Epoch 720, loss: 23.200\n",
      "Epoch 730, loss: 34.146\n",
      "Epoch 740, loss: 26.031\n",
      "Epoch 750, loss: 26.096\n",
      "Epoch 760, loss: 12.670\n",
      "Epoch 770, loss: 20.601\n",
      "Epoch 780, loss: 20.518\n",
      "Epoch 790, loss: 13.754\n",
      "Epoch 800, loss: 23.086\n",
      "Epoch 810, loss: 19.666\n",
      "Epoch 820, loss: 11.929\n",
      "Epoch 830, loss: 11.700\n",
      "Epoch 840, loss: 13.384\n",
      "Epoch 850, loss: 10.730\n",
      "Epoch 860, loss: 14.777\n",
      "Epoch 870, loss: 11.855\n",
      "Epoch 880, loss: 21.204\n",
      "Epoch 890, loss: 17.251\n",
      "Epoch 900, loss: 14.118\n",
      "Epoch 910, loss: 11.126\n",
      "Epoch 920, loss: 10.775\n",
      "Epoch 930, loss: 16.024\n",
      "Epoch 940, loss: 16.122\n",
      "Epoch 950, loss: 9.861\n",
      "Epoch 960, loss: 17.726\n",
      "Epoch 970, loss: 14.468\n",
      "Epoch 980, loss: 11.532\n",
      "Epoch 990, loss: 9.328\n",
      "Epoch 1000, loss: 16.963\n",
      "Epoch 1010, loss: 16.480\n",
      "Epoch 1020, loss: 11.613\n",
      "Epoch 1030, loss: 11.444\n",
      "Epoch 1040, loss: 8.467\n",
      "Epoch 1050, loss: 11.001\n",
      "Epoch 1060, loss: 10.486\n",
      "Epoch 1070, loss: 10.675\n",
      "Epoch 1080, loss: 10.269\n",
      "Epoch 1090, loss: 8.494\n",
      "Epoch 1100, loss: 11.081\n",
      "Epoch 1110, loss: 13.866\n",
      "Epoch 1120, loss: 10.269\n",
      "Epoch 1130, loss: 11.468\n",
      "Epoch 1140, loss: 12.089\n",
      "Epoch 1150, loss: 12.028\n",
      "Epoch 1160, loss: 7.908\n",
      "Epoch 1170, loss: 10.168\n",
      "Epoch 1180, loss: 10.348\n",
      "Epoch 1190, loss: 9.514\n",
      "Epoch 1200, loss: 7.790\n",
      "Epoch 1210, loss: 10.168\n",
      "Epoch 1220, loss: 8.259\n",
      "Epoch 1230, loss: 10.345\n",
      "Epoch 1240, loss: 8.249\n",
      "Epoch 1250, loss: 8.802\n",
      "Epoch 1260, loss: 7.700\n",
      "Epoch 1270, loss: 13.173\n",
      "Epoch 1280, loss: 10.361\n",
      "Epoch 1290, loss: 13.623\n",
      "Epoch 1300, loss: 9.029\n",
      "Epoch 1310, loss: 7.609\n",
      "Epoch 1320, loss: 8.136\n",
      "Epoch 1330, loss: 8.602\n",
      "Epoch 1340, loss: 9.160\n",
      "Epoch 1350, loss: 11.220\n",
      "Epoch 1360, loss: 12.616\n",
      "Epoch 1370, loss: 9.221\n",
      "Epoch 1380, loss: 10.075\n",
      "Epoch 1390, loss: 8.251\n",
      "Epoch 1400, loss: 7.900\n",
      "Epoch 1410, loss: 12.501\n",
      "Epoch 1420, loss: 11.280\n",
      "Epoch 1430, loss: 9.839\n",
      "Epoch 1440, loss: 7.900\n",
      "Epoch 1450, loss: 10.075\n",
      "Epoch 1460, loss: 10.754\n",
      "Epoch 1470, loss: 8.990\n",
      "Epoch 1480, loss: 10.869\n",
      "Epoch 1490, loss: 10.343\n",
      "Epoch 1500, loss: 11.280\n",
      "Epoch 1510, loss: 9.281\n",
      "Epoch 1520, loss: 9.198\n",
      "Epoch 1530, loss: 10.929\n",
      "Epoch 1540, loss: 8.311\n",
      "Epoch 1550, loss: 10.305\n",
      "Epoch 1560, loss: 8.075\n",
      "Epoch 1570, loss: 8.251\n",
      "Epoch 1580, loss: 7.900\n",
      "Epoch 1590, loss: 8.191\n",
      "Epoch 1600, loss: 9.960\n",
      "Epoch 1610, loss: 11.515\n",
      "Epoch 1620, loss: 10.130\n",
      "Epoch 1630, loss: 8.990\n",
      "Epoch 1640, loss: 8.777\n",
      "Epoch 1650, loss: 13.082\n",
      "Epoch 1660, loss: 8.893\n",
      "Epoch 1670, loss: 8.136\n",
      "Epoch 1680, loss: 8.893\n",
      "Epoch 1690, loss: 10.749\n",
      "Epoch 1700, loss: 8.953\n",
      "Epoch 1710, loss: 10.929\n",
      "Epoch 1720, loss: 8.579\n",
      "Epoch 1730, loss: 8.602\n",
      "Epoch 1740, loss: 12.616\n",
      "Epoch 1750, loss: 8.191\n",
      "Epoch 1760, loss: 10.195\n",
      "Epoch 1770, loss: 8.075\n",
      "Epoch 1780, loss: 10.190\n",
      "Epoch 1790, loss: 10.426\n",
      "Epoch 1800, loss: 9.281\n",
      "Epoch 1810, loss: 8.893\n",
      "Epoch 1820, loss: 8.929\n",
      "Epoch 1830, loss: 8.883\n",
      "Epoch 1840, loss: 10.434\n",
      "Epoch 1850, loss: 8.357\n",
      "Epoch 1860, loss: 9.164\n",
      "Epoch 1870, loss: 8.533\n",
      "Epoch 1880, loss: 10.661\n",
      "Epoch 1890, loss: 8.120\n",
      "Epoch 1900, loss: 10.208\n",
      "Epoch 1910, loss: 10.434\n",
      "Epoch 1920, loss: 10.259\n",
      "Epoch 1930, loss: 11.975\n",
      "Epoch 1940, loss: 8.419\n",
      "Epoch 1950, loss: 8.936\n",
      "Epoch 1960, loss: 8.068\n",
      "Epoch 1970, loss: 8.998\n",
      "Epoch 1980, loss: 8.874\n",
      "Epoch 1990, loss: 9.959\n",
      "Epoch 2000, loss: 9.390\n",
      "Epoch 2010, loss: 10.321\n",
      "Epoch 2020, loss: 12.117\n",
      "Epoch 2030, loss: 12.450\n",
      "Epoch 2040, loss: 9.455\n",
      "Epoch 2050, loss: 10.221\n",
      "Epoch 2060, loss: 7.930\n",
      "Epoch 2070, loss: 9.917\n",
      "Epoch 2080, loss: 11.749\n",
      "Epoch 2090, loss: 8.177\n",
      "Epoch 2100, loss: 10.507\n",
      "Epoch 2110, loss: 9.910\n",
      "Epoch 2120, loss: 9.094\n",
      "Epoch 2130, loss: 9.047\n",
      "Epoch 2140, loss: 10.046\n",
      "Epoch 2150, loss: 7.937\n",
      "Epoch 2160, loss: 10.314\n",
      "Epoch 2170, loss: 9.566\n",
      "Epoch 2180, loss: 7.651\n",
      "Epoch 2190, loss: 11.510\n",
      "Epoch 2200, loss: 8.177\n",
      "Epoch 2210, loss: 8.317\n",
      "Epoch 2220, loss: 8.843\n",
      "Epoch 2230, loss: 7.855\n",
      "Epoch 2240, loss: 9.112\n",
      "Epoch 2250, loss: 10.490\n",
      "Epoch 2260, loss: 8.872\n",
      "Epoch 2270, loss: 8.750\n",
      "Epoch 2280, loss: 7.651\n",
      "Epoch 2290, loss: 11.685\n",
      "Epoch 2300, loss: 10.332\n",
      "Epoch 2310, loss: 8.907\n",
      "Epoch 2320, loss: 10.203\n",
      "Epoch 2330, loss: 10.565\n",
      "Epoch 2340, loss: 9.613\n",
      "Epoch 2350, loss: 7.698\n",
      "Epoch 2360, loss: 7.411\n",
      "Epoch 2370, loss: 7.411\n",
      "Epoch 2380, loss: 8.843\n",
      "Epoch 2390, loss: 11.073\n",
      "Epoch 2400, loss: 7.236\n",
      "Epoch 2410, loss: 9.455\n",
      "Epoch 2420, loss: 9.047\n",
      "Epoch 2430, loss: 8.603\n",
      "Epoch 2440, loss: 11.248\n",
      "Epoch 2450, loss: 11.685\n",
      "Epoch 2460, loss: 11.685\n",
      "Epoch 2470, loss: 12.497\n",
      "Epoch 2480, loss: 12.497\n",
      "Epoch 2490, loss: 9.870\n",
      "Epoch 2500, loss: 9.065\n",
      "Epoch 2510, loss: 7.475\n",
      "Epoch 2520, loss: 9.151\n",
      "Epoch 2530, loss: 9.391\n",
      "Epoch 2540, loss: 8.464\n",
      "Epoch 2550, loss: 9.613\n",
      "Epoch 2560, loss: 9.742\n",
      "Epoch 2570, loss: 9.573\n",
      "Epoch 2580, loss: 7.762\n",
      "Epoch 2590, loss: 10.658\n",
      "Epoch 2600, loss: 8.206\n",
      "Epoch 2610, loss: 7.651\n",
      "Epoch 2620, loss: 12.211\n",
      "Epoch 2630, loss: 8.113\n",
      "Epoch 2640, loss: 10.722\n",
      "Epoch 2650, loss: 10.658\n",
      "Epoch 2660, loss: 10.565\n",
      "Epoch 2670, loss: 8.976\n",
      "Epoch 2680, loss: 10.028\n",
      "Epoch 2690, loss: 8.288\n",
      "Epoch 2700, loss: 9.677\n",
      "Epoch 2710, loss: 10.454\n",
      "Epoch 2720, loss: 9.047\n",
      "Epoch 2730, loss: 7.651\n",
      "Epoch 2740, loss: 12.497\n",
      "Epoch 2750, loss: 8.288\n",
      "Epoch 2760, loss: 9.545\n",
      "Epoch 2770, loss: 9.398\n",
      "Epoch 2780, loss: 9.373\n",
      "Epoch 2790, loss: 10.781\n",
      "Epoch 2800, loss: 9.360\n",
      "Epoch 2810, loss: 8.622\n",
      "Epoch 2820, loss: 10.504\n",
      "Epoch 2830, loss: 10.112\n",
      "Epoch 2840, loss: 8.395\n",
      "Epoch 2850, loss: 11.554\n",
      "Epoch 2860, loss: 7.461\n",
      "Epoch 2870, loss: 10.494\n",
      "Epoch 2880, loss: 9.398\n",
      "Epoch 2890, loss: 7.648\n",
      "Epoch 2900, loss: 8.786\n",
      "Epoch 2910, loss: 9.507\n",
      "Epoch 2920, loss: 7.539\n",
      "Epoch 2930, loss: 9.749\n",
      "Epoch 2940, loss: 9.090\n",
      "Epoch 2950, loss: 10.645\n",
      "Epoch 2960, loss: 7.998\n",
      "Epoch 2970, loss: 7.297\n",
      "Epoch 2980, loss: 7.363\n",
      "Epoch 2990, loss: 7.932\n",
      "Epoch 3000, loss: 10.494\n",
      "Epoch 3010, loss: 9.507\n",
      "Epoch 3020, loss: 8.588\n",
      "Epoch 3030, loss: 9.968\n",
      "Epoch 3040, loss: 8.240\n",
      "Epoch 3050, loss: 8.786\n",
      "Epoch 3060, loss: 8.696\n",
      "Epoch 3070, loss: 8.853\n",
      "Epoch 3080, loss: 8.338\n",
      "Epoch 3090, loss: 8.696\n",
      "Epoch 3100, loss: 9.991\n",
      "Epoch 3110, loss: 9.507\n",
      "Epoch 3120, loss: 9.487\n",
      "Epoch 3130, loss: 9.968\n",
      "Epoch 3140, loss: 8.853\n",
      "Epoch 3150, loss: 7.648\n",
      "Epoch 3160, loss: 8.568\n",
      "Epoch 3170, loss: 8.174\n",
      "Epoch 3180, loss: 10.645\n",
      "Epoch 3190, loss: 10.033\n",
      "Epoch 3200, loss: 7.363\n",
      "Epoch 3210, loss: 9.991\n",
      "Epoch 3220, loss: 8.338\n",
      "Epoch 3230, loss: 9.379\n",
      "Epoch 3240, loss: 8.054\n",
      "Epoch 3250, loss: 10.711\n",
      "Epoch 3260, loss: 9.640\n",
      "Epoch 3270, loss: 8.755\n",
      "Epoch 3280, loss: 7.890\n",
      "Epoch 3290, loss: 9.550\n",
      "Epoch 3300, loss: 9.465\n",
      "Epoch 3310, loss: 7.998\n",
      "Epoch 3320, loss: 7.890\n",
      "Epoch 3330, loss: 9.815\n",
      "Epoch 3340, loss: 7.998\n",
      "Epoch 3350, loss: 8.700\n",
      "Epoch 3360, loss: 10.603\n",
      "Epoch 3370, loss: 9.663\n",
      "Epoch 3380, loss: 7.678\n",
      "Epoch 3390, loss: 9.440\n",
      "Epoch 3400, loss: 9.465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3410, loss: 9.398\n",
      "Epoch 3420, loss: 8.018\n",
      "Epoch 3430, loss: 10.669\n",
      "Epoch 3440, loss: 9.356\n",
      "Epoch 3450, loss: 8.981\n",
      "Epoch 3460, loss: 9.070\n",
      "Epoch 3470, loss: 11.662\n",
      "Epoch 3480, loss: 7.539\n",
      "Epoch 3490, loss: 10.275\n",
      "Epoch 3500, loss: 11.554\n",
      "Epoch 3510, loss: 9.858\n",
      "Epoch 3520, loss: 11.554\n",
      "Epoch 3530, loss: 9.180\n",
      "Epoch 3540, loss: 12.363\n",
      "Epoch 3550, loss: 10.033\n",
      "Epoch 3560, loss: 8.447\n",
      "Epoch 3570, loss: 8.404\n",
      "Epoch 3580, loss: 7.932\n",
      "Epoch 3590, loss: 9.223\n",
      "Epoch 3600, loss: 12.363\n",
      "Epoch 3610, loss: 9.815\n",
      "Epoch 3620, loss: 7.756\n",
      "Epoch 3630, loss: 8.107\n",
      "Epoch 3640, loss: 10.961\n",
      "Epoch 3650, loss: 12.013\n",
      "Epoch 3660, loss: 10.778\n",
      "Epoch 3670, loss: 8.677\n",
      "Epoch 3680, loss: 8.853\n",
      "Epoch 3690, loss: 9.247\n",
      "Epoch 3700, loss: 10.603\n",
      "Epoch 3710, loss: 9.223\n",
      "Epoch 3720, loss: 8.041\n",
      "Epoch 3730, loss: 10.736\n",
      "Epoch 3740, loss: 7.363\n",
      "Epoch 3750, loss: 8.084\n",
      "Epoch 3760, loss: 8.622\n",
      "Epoch 3770, loss: 8.018\n",
      "Epoch 3780, loss: 9.659\n",
      "Epoch 3790, loss: 7.932\n",
      "Epoch 3800, loss: 9.924\n",
      "Epoch 3810, loss: 10.603\n",
      "Epoch 3820, loss: 9.223\n",
      "Epoch 3830, loss: 9.682\n",
      "Epoch 3840, loss: 7.363\n",
      "Epoch 3850, loss: 7.648\n",
      "Epoch 3860, loss: 10.210\n",
      "Epoch 3870, loss: 9.487\n",
      "Epoch 3880, loss: 8.404\n",
      "Epoch 3890, loss: 11.554\n",
      "Epoch 3900, loss: 9.114\n",
      "Epoch 3910, loss: 8.633\n",
      "Epoch 3920, loss: 10.603\n",
      "Epoch 3930, loss: 7.998\n",
      "Epoch 3940, loss: 7.539\n",
      "Epoch 3950, loss: 7.363\n",
      "Epoch 3960, loss: 9.028\n",
      "Epoch 3970, loss: 10.185\n",
      "Epoch 3980, loss: 10.077\n",
      "Epoch 3990, loss: 10.450\n",
      "Epoch 4000, loss: 9.203\n",
      "Epoch 4010, loss: 9.815\n",
      "Epoch 4020, loss: 7.932\n",
      "Epoch 4030, loss: 7.998\n",
      "Epoch 4040, loss: 9.090\n",
      "Epoch 4050, loss: 9.924\n",
      "Epoch 4060, loss: 7.823\n",
      "Epoch 4070, loss: 8.696\n",
      "Epoch 4080, loss: 12.363\n",
      "Epoch 4090, loss: 10.099\n",
      "Epoch 4100, loss: 9.773\n",
      "Epoch 4110, loss: 7.539\n",
      "Epoch 4120, loss: 7.998\n",
      "Epoch 4130, loss: 9.070\n",
      "Epoch 4140, loss: 8.700\n",
      "Epoch 4150, loss: 7.998\n",
      "Epoch 4160, loss: 9.706\n",
      "Epoch 4170, loss: 9.749\n",
      "Epoch 4180, loss: 9.356\n",
      "Epoch 4190, loss: 9.640\n",
      "Epoch 4200, loss: 8.162\n",
      "Epoch 4210, loss: 7.539\n",
      "Epoch 4220, loss: 9.991\n",
      "Epoch 4230, loss: 10.057\n",
      "Epoch 4240, loss: 9.706\n",
      "Epoch 4250, loss: 11.554\n",
      "Epoch 4260, loss: 9.507\n",
      "Epoch 4270, loss: 7.998\n",
      "Epoch 4280, loss: 9.640\n",
      "Epoch 4290, loss: 8.853\n",
      "Epoch 4300, loss: 9.598\n",
      "Epoch 4310, loss: 8.174\n",
      "Epoch 4320, loss: 8.689\n",
      "Epoch 4330, loss: 8.174\n",
      "Epoch 4340, loss: 7.472\n",
      "Epoch 4350, loss: 8.919\n",
      "Epoch 4360, loss: 9.924\n",
      "Epoch 4370, loss: 7.121\n",
      "Epoch 4380, loss: 7.890\n",
      "Epoch 4390, loss: 8.622\n",
      "Epoch 4400, loss: 9.039\n",
      "Epoch 4410, loss: 7.297\n",
      "Epoch 4420, loss: 7.678\n",
      "Epoch 4430, loss: 9.265\n",
      "Epoch 4440, loss: 7.406\n",
      "Epoch 4450, loss: 7.890\n",
      "Epoch 4460, loss: 10.057\n",
      "Epoch 4470, loss: 7.823\n",
      "Epoch 4480, loss: 9.991\n",
      "Epoch 4490, loss: 8.240\n",
      "Epoch 4500, loss: 8.174\n",
      "Epoch 4510, loss: 9.465\n",
      "Epoch 4520, loss: 9.507\n",
      "Epoch 4530, loss: 8.568\n",
      "Epoch 4540, loss: 9.640\n",
      "Epoch 4550, loss: 8.689\n",
      "Epoch 4560, loss: 11.554\n",
      "Epoch 4570, loss: 9.991\n",
      "Epoch 4580, loss: 10.494\n",
      "Epoch 4590, loss: 8.872\n",
      "Epoch 4600, loss: 9.114\n",
      "Epoch 4610, loss: 8.525\n",
      "Epoch 4620, loss: 9.070\n",
      "Epoch 4630, loss: 9.598\n",
      "Epoch 4640, loss: 10.786\n",
      "Epoch 4650, loss: 10.517\n",
      "Epoch 4660, loss: 8.763\n",
      "Epoch 4670, loss: 7.998\n",
      "Epoch 4680, loss: 9.398\n",
      "Epoch 4690, loss: 11.662\n",
      "Epoch 4700, loss: 9.028\n",
      "Epoch 4710, loss: 8.633\n",
      "Epoch 4720, loss: 9.465\n",
      "Epoch 4730, loss: 9.379\n",
      "Epoch 4740, loss: 11.729\n",
      "Epoch 4750, loss: 7.363\n",
      "Epoch 4760, loss: 7.012\n",
      "Epoch 4770, loss: 8.326\n",
      "Epoch 4780, loss: 11.904\n",
      "Epoch 4790, loss: 7.932\n",
      "Epoch 4800, loss: 9.070\n",
      "Epoch 4810, loss: 8.700\n",
      "Epoch 4820, loss: 7.932\n",
      "Epoch 4830, loss: 10.953\n",
      "Epoch 4840, loss: 7.539\n",
      "Epoch 4850, loss: 10.057\n",
      "Epoch 4860, loss: 8.719\n",
      "Epoch 4870, loss: 9.398\n",
      "Epoch 4880, loss: 8.084\n",
      "Epoch 4890, loss: 9.815\n",
      "Epoch 4900, loss: 8.174\n",
      "Epoch 4910, loss: 8.633\n",
      "Epoch 4920, loss: 7.406\n",
      "Epoch 4930, loss: 9.968\n",
      "Epoch 4940, loss: 9.114\n",
      "Epoch 4950, loss: 9.005\n",
      "Epoch 4960, loss: 9.247\n",
      "Epoch 4970, loss: 8.696\n",
      "Epoch 4980, loss: 7.188\n",
      "Epoch 4990, loss: 9.640\n",
      "Epoch 5000, loss: 7.648\n",
      "Epoch 5010, loss: 7.998\n",
      "Epoch 5020, loss: 8.864\n",
      "Epoch 5030, loss: 12.430\n",
      "Epoch 5040, loss: 8.938\n",
      "Epoch 5050, loss: 8.544\n",
      "Epoch 5060, loss: 8.513\n",
      "Epoch 5070, loss: 7.678\n",
      "Epoch 5080, loss: 10.494\n",
      "Epoch 5090, loss: 8.107\n",
      "Epoch 5100, loss: 9.706\n",
      "Epoch 5110, loss: 11.270\n",
      "Epoch 5120, loss: 10.778\n",
      "Epoch 5130, loss: 9.598\n",
      "Epoch 5140, loss: 11.445\n",
      "Epoch 5150, loss: 7.363\n",
      "Epoch 5160, loss: 8.054\n",
      "Epoch 5170, loss: 9.616\n",
      "Epoch 5180, loss: 8.458\n",
      "Epoch 5190, loss: 7.297\n",
      "Epoch 5200, loss: 7.012\n",
      "Epoch 5210, loss: 8.349\n",
      "Epoch 5220, loss: 9.356\n",
      "Epoch 5230, loss: 8.054\n",
      "Epoch 5240, loss: 7.998\n",
      "Epoch 5250, loss: 10.536\n",
      "Epoch 5260, loss: 8.875\n",
      "Epoch 5270, loss: 9.180\n",
      "Epoch 5280, loss: 8.084\n",
      "Epoch 5290, loss: 8.973\n",
      "Epoch 5300, loss: 8.633\n",
      "Epoch 5310, loss: 10.961\n",
      "Epoch 5320, loss: 7.812\n",
      "Epoch 5330, loss: 11.028\n",
      "Epoch 5340, loss: 11.270\n",
      "Epoch 5350, loss: 7.998\n",
      "Epoch 5360, loss: 11.203\n",
      "Epoch 5370, loss: 9.465\n",
      "Epoch 5380, loss: 7.363\n",
      "Epoch 5390, loss: 7.932\n",
      "Epoch 5400, loss: 7.363\n",
      "Epoch 5410, loss: 7.932\n",
      "Epoch 5420, loss: 9.005\n",
      "Epoch 5430, loss: 8.458\n",
      "Epoch 5440, loss: 9.991\n",
      "Epoch 5450, loss: 9.991\n",
      "Epoch 5460, loss: 7.932\n",
      "Epoch 5470, loss: 10.210\n",
      "Epoch 5480, loss: 9.465\n",
      "Epoch 5490, loss: 8.568\n",
      "Epoch 5500, loss: 9.991\n",
      "Epoch 5510, loss: 7.363\n",
      "Epoch 5520, loss: 7.823\n",
      "Epoch 5530, loss: 11.270\n",
      "Epoch 5540, loss: 7.823\n",
      "Epoch 5550, loss: 10.233\n",
      "Epoch 5560, loss: 7.012\n",
      "Epoch 5570, loss: 7.998\n",
      "Epoch 5580, loss: 7.297\n",
      "Epoch 5590, loss: 9.090\n",
      "Epoch 5600, loss: 9.924\n",
      "Epoch 5610, loss: 8.205\n",
      "Epoch 5620, loss: 9.422\n",
      "Epoch 5630, loss: 8.622\n",
      "Epoch 5640, loss: 9.924\n",
      "Epoch 5650, loss: 8.938\n",
      "Epoch 5660, loss: 9.156\n",
      "Epoch 5670, loss: 12.013\n",
      "Epoch 5680, loss: 10.099\n",
      "Epoch 5690, loss: 9.901\n",
      "Epoch 5700, loss: 9.882\n",
      "Epoch 5710, loss: 8.786\n",
      "Epoch 5720, loss: 12.430\n",
      "Epoch 5730, loss: 10.033\n",
      "Epoch 5740, loss: 9.749\n",
      "Epoch 5750, loss: 9.156\n",
      "Epoch 5760, loss: 7.363\n",
      "Epoch 5770, loss: 7.920\n",
      "Epoch 5780, loss: 8.193\n",
      "Epoch 5790, loss: 10.294\n",
      "Epoch 5800, loss: 8.349\n",
      "Epoch 5810, loss: 9.422\n",
      "Epoch 5820, loss: 8.961\n",
      "Epoch 5830, loss: 10.033\n",
      "Epoch 5840, loss: 8.872\n",
      "Epoch 5850, loss: 9.223\n",
      "Epoch 5860, loss: 7.648\n",
      "Epoch 5870, loss: 8.895\n",
      "Epoch 5880, loss: 9.247\n",
      "Epoch 5890, loss: 7.539\n",
      "Epoch 5900, loss: 7.121\n",
      "Epoch 5910, loss: 7.297\n",
      "Epoch 5920, loss: 9.203\n",
      "Epoch 5930, loss: 10.778\n",
      "Epoch 5940, loss: 8.174\n",
      "Epoch 5950, loss: 9.507\n",
      "Epoch 5960, loss: 9.598\n",
      "Epoch 5970, loss: 9.070\n",
      "Epoch 5980, loss: 8.525\n",
      "Epoch 5990, loss: 8.633\n",
      "Epoch 6000, loss: 7.998\n",
      "Epoch 6010, loss: 11.729\n",
      "Epoch 6020, loss: 10.494\n",
      "Epoch 6030, loss: 7.363\n",
      "Epoch 6040, loss: 7.823\n",
      "Epoch 6050, loss: 8.205\n",
      "Epoch 6060, loss: 9.223\n",
      "Epoch 6070, loss: 7.297\n",
      "Epoch 6080, loss: 7.055\n",
      "Epoch 6090, loss: 8.872\n",
      "Epoch 6100, loss: 9.659\n",
      "Epoch 6110, loss: 8.447\n",
      "Epoch 6120, loss: 10.494\n",
      "Epoch 6130, loss: 10.233\n",
      "Epoch 6140, loss: 9.659\n",
      "Epoch 6150, loss: 7.297\n",
      "Epoch 6160, loss: 7.998\n",
      "Epoch 6170, loss: 9.659\n",
      "Epoch 6180, loss: 9.507\n",
      "Epoch 6190, loss: 11.554\n",
      "Epoch 6200, loss: 7.998\n",
      "Epoch 6210, loss: 8.630\n",
      "Epoch 6220, loss: 8.719\n",
      "Epoch 6230, loss: 8.633\n",
      "Epoch 6240, loss: 8.513\n",
      "Epoch 6250, loss: 7.297\n",
      "Epoch 6260, loss: 7.363\n",
      "Epoch 6270, loss: 10.185\n",
      "Epoch 6280, loss: 11.554\n",
      "Epoch 6290, loss: 7.998\n",
      "Epoch 6300, loss: 7.823\n",
      "Epoch 6310, loss: 12.013\n",
      "Epoch 6320, loss: 12.013\n",
      "Epoch 6330, loss: 7.932\n",
      "Epoch 6340, loss: 11.203\n",
      "Epoch 6350, loss: 7.188\n",
      "Epoch 6360, loss: 10.341\n",
      "Epoch 6370, loss: 10.361\n",
      "Epoch 6380, loss: 10.143\n",
      "Epoch 6390, loss: 9.991\n",
      "Epoch 6400, loss: 11.312\n",
      "Epoch 6410, loss: 7.363\n",
      "Epoch 6420, loss: 9.991\n",
      "Epoch 6430, loss: 12.013\n",
      "Epoch 6440, loss: 9.815\n",
      "Epoch 6450, loss: 8.447\n",
      "Epoch 6460, loss: 8.349\n",
      "Epoch 6470, loss: 9.598\n",
      "Epoch 6480, loss: 7.648\n",
      "Epoch 6490, loss: 7.714\n",
      "Epoch 6500, loss: 7.539\n",
      "Epoch 6510, loss: 8.054\n",
      "Epoch 6520, loss: 9.968\n",
      "Epoch 6530, loss: 9.968\n",
      "Epoch 6540, loss: 8.700\n",
      "Epoch 6550, loss: 9.682\n",
      "Epoch 6560, loss: 9.005\n",
      "Epoch 6570, loss: 9.356\n",
      "Epoch 6580, loss: 9.421\n",
      "Epoch 6590, loss: 9.749\n",
      "Epoch 6600, loss: 8.786\n",
      "Epoch 6610, loss: 9.968\n",
      "Epoch 6620, loss: 9.598\n",
      "Epoch 6630, loss: 8.872\n",
      "Epoch 6640, loss: 10.233\n",
      "Epoch 6650, loss: 8.084\n",
      "Epoch 6660, loss: 11.904\n",
      "Epoch 6670, loss: 7.823\n",
      "Epoch 6680, loss: 11.554\n",
      "Epoch 6690, loss: 10.077\n",
      "Epoch 6700, loss: 11.904\n",
      "Epoch 6710, loss: 11.554\n",
      "Epoch 6720, loss: 10.961\n",
      "Epoch 6730, loss: 7.909\n",
      "Epoch 6740, loss: 9.773\n",
      "Epoch 6750, loss: 8.853\n",
      "Epoch 6760, loss: 9.682\n",
      "Epoch 6770, loss: 9.507\n",
      "Epoch 6780, loss: 9.663\n",
      "Epoch 6790, loss: 10.210\n",
      "Epoch 6800, loss: 9.616\n",
      "Epoch 6810, loss: 8.283\n",
      "Epoch 6820, loss: 7.998\n",
      "Epoch 6830, loss: 8.633\n",
      "Epoch 6840, loss: 8.755\n",
      "Epoch 6850, loss: 9.422\n",
      "Epoch 6860, loss: 10.754\n",
      "Epoch 6870, loss: 8.938\n",
      "Epoch 6880, loss: 7.823\n",
      "Epoch 6890, loss: 9.550\n",
      "Epoch 6900, loss: 8.349\n",
      "Epoch 6910, loss: 8.174\n",
      "Epoch 6920, loss: 9.948\n",
      "Epoch 6930, loss: 10.210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6940, loss: 11.445\n",
      "Epoch 6950, loss: 7.932\n",
      "Epoch 6960, loss: 8.875\n",
      "Epoch 6970, loss: 8.864\n",
      "Epoch 6980, loss: 10.494\n",
      "Epoch 6990, loss: 7.297\n",
      "Epoch 7000, loss: 9.640\n",
      "Epoch 7010, loss: 8.349\n",
      "Epoch 7020, loss: 8.151\n",
      "Epoch 7030, loss: 10.208\n",
      "Epoch 7040, loss: 9.070\n",
      "Epoch 7050, loss: 9.247\n",
      "Epoch 7060, loss: 9.968\n",
      "Epoch 7070, loss: 8.938\n",
      "Epoch 7080, loss: 8.174\n",
      "Epoch 7090, loss: 8.151\n",
      "Epoch 7100, loss: 8.127\n",
      "Epoch 7110, loss: 9.398\n",
      "Epoch 7120, loss: 7.998\n",
      "Epoch 7130, loss: 7.998\n",
      "Epoch 7140, loss: 8.349\n",
      "Epoch 7150, loss: 11.904\n",
      "Epoch 7160, loss: 8.240\n",
      "Epoch 7170, loss: 12.255\n",
      "Epoch 7180, loss: 9.815\n",
      "Epoch 7190, loss: 8.349\n",
      "Epoch 7200, loss: 10.185\n",
      "Epoch 7210, loss: 9.659\n",
      "Epoch 7220, loss: 8.127\n",
      "Epoch 7230, loss: 8.696\n",
      "Epoch 7240, loss: 8.502\n",
      "Epoch 7250, loss: 7.823\n",
      "Epoch 7260, loss: 7.890\n",
      "Epoch 7270, loss: 7.823\n",
      "Epoch 7280, loss: 7.823\n",
      "Epoch 7290, loss: 7.055\n",
      "Epoch 7300, loss: 9.815\n",
      "Epoch 7310, loss: 9.398\n",
      "Epoch 7320, loss: 10.294\n",
      "Epoch 7330, loss: 8.689\n",
      "Epoch 7340, loss: 10.736\n",
      "Epoch 7350, loss: 7.890\n",
      "Epoch 7360, loss: 9.991\n",
      "Epoch 7370, loss: 8.961\n",
      "Epoch 7380, loss: 9.550\n",
      "Epoch 7390, loss: 7.823\n",
      "Epoch 7400, loss: 8.127\n",
      "Epoch 7410, loss: 9.487\n",
      "Epoch 7420, loss: 8.689\n",
      "Epoch 7430, loss: 9.356\n",
      "Epoch 7440, loss: 9.773\n",
      "Epoch 7450, loss: 9.331\n",
      "Epoch 7460, loss: 9.640\n",
      "Epoch 7470, loss: 11.445\n",
      "Epoch 7480, loss: 8.174\n",
      "Epoch 7490, loss: 11.729\n",
      "Epoch 7500, loss: 10.099\n",
      "Epoch 7510, loss: 7.297\n",
      "Epoch 7520, loss: 9.598\n",
      "Epoch 7530, loss: 11.203\n",
      "Epoch 7540, loss: 10.736\n",
      "Epoch 7550, loss: 8.513\n",
      "Epoch 7560, loss: 8.872\n",
      "Epoch 7570, loss: 8.544\n",
      "Epoch 7580, loss: 8.174\n",
      "Epoch 7590, loss: 8.174\n",
      "Epoch 7600, loss: 11.729\n",
      "Epoch 7610, loss: 7.854\n",
      "Epoch 7620, loss: 7.998\n",
      "Epoch 7630, loss: 7.823\n",
      "Epoch 7640, loss: 8.853\n",
      "Epoch 7650, loss: 7.890\n",
      "Epoch 7660, loss: 9.422\n",
      "Epoch 7670, loss: 11.312\n",
      "Epoch 7680, loss: 10.033\n",
      "Epoch 7690, loss: 7.406\n",
      "Epoch 7700, loss: 8.338\n",
      "Epoch 7710, loss: 8.853\n",
      "Epoch 7720, loss: 8.755\n",
      "Epoch 7730, loss: 7.012\n",
      "Epoch 7740, loss: 10.210\n",
      "Epoch 7750, loss: 10.961\n",
      "Epoch 7760, loss: 7.890\n",
      "Epoch 7770, loss: 9.991\n",
      "Epoch 7780, loss: 9.356\n",
      "Epoch 7790, loss: 10.119\n",
      "Epoch 7800, loss: 9.573\n",
      "Epoch 7810, loss: 11.312\n",
      "Epoch 7820, loss: 9.039\n",
      "Epoch 7830, loss: 7.363\n",
      "Epoch 7840, loss: 9.598\n",
      "Epoch 7850, loss: 9.882\n",
      "Epoch 7860, loss: 9.991\n",
      "Epoch 7870, loss: 11.904\n",
      "Epoch 7880, loss: 7.932\n",
      "Epoch 7890, loss: 9.749\n",
      "Epoch 7900, loss: 9.815\n",
      "Epoch 7910, loss: 10.057\n",
      "Epoch 7920, loss: 7.823\n",
      "Epoch 7930, loss: 9.356\n",
      "Epoch 7940, loss: 8.174\n",
      "Epoch 7950, loss: 7.890\n",
      "Epoch 7960, loss: 9.598\n",
      "Epoch 7970, loss: 8.981\n",
      "Epoch 7980, loss: 9.706\n",
      "Epoch 7990, loss: 7.678\n",
      "Epoch 8000, loss: 8.689\n",
      "Epoch 8010, loss: 7.297\n",
      "Epoch 8020, loss: 7.461\n",
      "Epoch 8030, loss: 8.914\n",
      "Epoch 8040, loss: 8.065\n",
      "Epoch 8050, loss: 10.341\n",
      "Epoch 8060, loss: 8.065\n",
      "Epoch 8070, loss: 7.890\n",
      "Epoch 8080, loss: 7.890\n",
      "Epoch 8090, loss: 9.356\n",
      "Epoch 8100, loss: 10.427\n",
      "Epoch 8110, loss: 7.812\n",
      "Epoch 8120, loss: 10.494\n",
      "Epoch 8130, loss: 9.398\n",
      "Epoch 8140, loss: 7.406\n",
      "Epoch 8150, loss: 10.143\n",
      "Epoch 8160, loss: 9.598\n",
      "Epoch 8170, loss: 11.904\n",
      "Epoch 8180, loss: 9.356\n",
      "Epoch 8190, loss: 7.920\n",
      "Epoch 8200, loss: 9.815\n",
      "Epoch 8210, loss: 8.174\n",
      "Epoch 8220, loss: 9.203\n",
      "Epoch 8230, loss: 7.920\n",
      "Epoch 8240, loss: 7.823\n",
      "Epoch 8250, loss: 9.114\n",
      "Epoch 8260, loss: 7.998\n",
      "Epoch 8270, loss: 10.099\n",
      "Epoch 8280, loss: 9.659\n",
      "Epoch 8290, loss: 11.270\n",
      "Epoch 8300, loss: 9.507\n",
      "Epoch 8310, loss: 8.174\n",
      "Epoch 8320, loss: 10.294\n",
      "Epoch 8330, loss: 9.005\n",
      "Epoch 8340, loss: 7.890\n",
      "Epoch 8350, loss: 11.554\n",
      "Epoch 8360, loss: 8.513\n",
      "Epoch 8370, loss: 10.233\n",
      "Epoch 8380, loss: 8.349\n",
      "Epoch 8390, loss: 8.513\n",
      "Epoch 8400, loss: 7.998\n",
      "Epoch 8410, loss: 10.166\n",
      "Epoch 8420, loss: 8.174\n",
      "Epoch 8430, loss: 9.487\n",
      "Epoch 8440, loss: 10.099\n",
      "Epoch 8450, loss: 8.677\n",
      "Epoch 8460, loss: 9.901\n",
      "Epoch 8470, loss: 9.421\n",
      "Epoch 8480, loss: 9.356\n",
      "Epoch 8490, loss: 9.247\n",
      "Epoch 8500, loss: 8.633\n",
      "Epoch 8510, loss: 9.991\n",
      "Epoch 8520, loss: 8.174\n",
      "Epoch 8530, loss: 7.932\n",
      "Epoch 8540, loss: 7.297\n",
      "Epoch 8550, loss: 9.331\n",
      "Epoch 8560, loss: 8.447\n",
      "Epoch 8570, loss: 10.603\n",
      "Epoch 8580, loss: 9.948\n",
      "Epoch 8590, loss: 8.174\n",
      "Epoch 8600, loss: 8.502\n",
      "Epoch 8610, loss: 8.689\n",
      "Epoch 8620, loss: 10.185\n",
      "Epoch 8630, loss: 8.633\n",
      "Epoch 8640, loss: 12.079\n",
      "Epoch 8650, loss: 10.603\n",
      "Epoch 8660, loss: 10.233\n",
      "Epoch 8670, loss: 8.283\n",
      "Epoch 8680, loss: 9.070\n",
      "Epoch 8690, loss: 11.312\n",
      "Epoch 8700, loss: 7.998\n",
      "Epoch 8710, loss: 9.507\n",
      "Epoch 8720, loss: 8.283\n",
      "Epoch 8730, loss: 7.823\n",
      "Epoch 8740, loss: 7.297\n",
      "Epoch 8750, loss: 10.786\n",
      "Epoch 8760, loss: 8.525\n",
      "Epoch 8770, loss: 8.349\n",
      "Epoch 8780, loss: 11.312\n",
      "Epoch 8790, loss: 8.786\n",
      "Epoch 8800, loss: 8.240\n",
      "Epoch 8810, loss: 9.749\n",
      "Epoch 8820, loss: 11.028\n",
      "Epoch 8830, loss: 9.223\n",
      "Epoch 8840, loss: 8.458\n",
      "Epoch 8850, loss: 9.815\n",
      "Epoch 8860, loss: 11.270\n",
      "Epoch 8870, loss: 7.539\n",
      "Epoch 8880, loss: 11.270\n",
      "Epoch 8890, loss: 7.539\n",
      "Epoch 8900, loss: 6.946\n",
      "Epoch 8910, loss: 7.823\n",
      "Epoch 8920, loss: 11.554\n",
      "Epoch 8930, loss: 8.633\n",
      "Epoch 8940, loss: 8.633\n",
      "Epoch 8950, loss: 10.603\n",
      "Epoch 8960, loss: 9.422\n",
      "Epoch 8970, loss: 8.938\n",
      "Epoch 8980, loss: 9.749\n",
      "Epoch 8990, loss: 8.174\n",
      "Epoch 9000, loss: 8.174\n",
      "Epoch 9010, loss: 11.094\n",
      "Epoch 9020, loss: 9.247\n",
      "Epoch 9030, loss: 11.662\n",
      "Epoch 9040, loss: 7.932\n",
      "Epoch 9050, loss: 8.217\n",
      "Epoch 9060, loss: 9.465\n",
      "Epoch 9070, loss: 8.700\n",
      "Epoch 9080, loss: 7.932\n",
      "Epoch 9090, loss: 9.309\n",
      "Epoch 9100, loss: 10.778\n",
      "Epoch 9110, loss: 7.890\n",
      "Epoch 9120, loss: 7.890\n",
      "Epoch 9130, loss: 7.920\n",
      "Epoch 9140, loss: 9.815\n",
      "Epoch 9150, loss: 8.919\n",
      "Epoch 9160, loss: 7.714\n",
      "Epoch 9170, loss: 7.570\n",
      "Epoch 9180, loss: 11.270\n",
      "Epoch 9190, loss: 7.570\n",
      "Epoch 9200, loss: 10.119\n",
      "Epoch 9210, loss: 7.932\n",
      "Epoch 9220, loss: 8.677\n",
      "Epoch 9230, loss: 10.252\n",
      "Epoch 9240, loss: 9.924\n",
      "Epoch 9250, loss: 8.054\n",
      "Epoch 9260, loss: 8.544\n",
      "Epoch 9270, loss: 9.356\n",
      "Epoch 9280, loss: 8.369\n",
      "Epoch 9290, loss: 9.968\n",
      "Epoch 9300, loss: 7.890\n",
      "Epoch 9310, loss: 8.349\n",
      "Epoch 9320, loss: 7.998\n",
      "Epoch 9330, loss: 9.507\n",
      "Epoch 9340, loss: 7.812\n",
      "Epoch 9350, loss: 9.991\n",
      "Epoch 9360, loss: 10.408\n",
      "Epoch 9370, loss: 8.151\n",
      "Epoch 9380, loss: 9.223\n",
      "Epoch 9390, loss: 8.797\n",
      "Epoch 9400, loss: 10.143\n",
      "Epoch 9410, loss: 9.640\n",
      "Epoch 9420, loss: 11.445\n",
      "Epoch 9430, loss: 10.185\n",
      "Epoch 9440, loss: 8.568\n",
      "Epoch 9450, loss: 9.991\n",
      "Epoch 9460, loss: 9.659\n",
      "Epoch 9470, loss: 9.882\n",
      "Epoch 9480, loss: 9.422\n",
      "Epoch 9490, loss: 10.185\n",
      "Epoch 9500, loss: 8.054\n",
      "Epoch 9510, loss: 8.568\n",
      "Epoch 9520, loss: 8.805\n",
      "Epoch 9530, loss: 9.223\n",
      "Epoch 9540, loss: 9.573\n",
      "Epoch 9550, loss: 7.812\n",
      "Epoch 9560, loss: 8.872\n",
      "Epoch 9570, loss: 7.998\n",
      "Epoch 9580, loss: 9.114\n",
      "Epoch 9590, loss: 10.033\n",
      "Epoch 9600, loss: 9.640\n",
      "Epoch 9610, loss: 9.465\n",
      "Epoch 9620, loss: 10.119\n",
      "Epoch 9630, loss: 6.946\n",
      "Epoch 9640, loss: 8.872\n",
      "Epoch 9650, loss: 9.640\n",
      "Epoch 9660, loss: 9.487\n",
      "Epoch 9670, loss: 7.998\n",
      "Epoch 9680, loss: 8.349\n",
      "Epoch 9690, loss: 11.203\n",
      "Epoch 9700, loss: 9.991\n",
      "Epoch 9710, loss: 9.640\n",
      "Epoch 9720, loss: 7.998\n",
      "Epoch 9730, loss: 9.356\n",
      "Epoch 9740, loss: 7.998\n",
      "Epoch 9750, loss: 8.174\n",
      "Epoch 9760, loss: 11.729\n",
      "Epoch 9770, loss: 9.991\n",
      "Epoch 9780, loss: 11.729\n",
      "Epoch 9790, loss: 9.180\n",
      "Epoch 9800, loss: 8.786\n",
      "Epoch 9810, loss: 7.297\n",
      "Epoch 9820, loss: 7.703\n",
      "Epoch 9830, loss: 9.507\n",
      "Epoch 9840, loss: 9.815\n",
      "Epoch 9850, loss: 8.906\n",
      "Epoch 9860, loss: 9.114\n",
      "Epoch 9870, loss: 10.450\n",
      "Epoch 9880, loss: 8.502\n",
      "Epoch 9890, loss: 8.205\n",
      "Epoch 9900, loss: 7.823\n",
      "Epoch 9910, loss: 11.094\n",
      "Epoch 9920, loss: 9.356\n",
      "Epoch 9930, loss: 8.696\n",
      "Epoch 9940, loss: 9.991\n",
      "Epoch 9950, loss: 8.961\n",
      "Epoch 9960, loss: 7.756\n",
      "Epoch 9970, loss: 9.223\n",
      "Epoch 9980, loss: 8.872\n",
      "Epoch 9990, loss: 8.919\n",
      "Epoch 10000, loss: 9.465\n",
      "Epoch 10010, loss: 11.445\n",
      "Epoch 10020, loss: 7.823\n",
      "Epoch 10030, loss: 9.090\n",
      "Epoch 10040, loss: 7.920\n",
      "Epoch 10050, loss: 9.682\n",
      "Epoch 10060, loss: 10.077\n",
      "Epoch 10070, loss: 11.904\n",
      "Epoch 10080, loss: 9.114\n",
      "Epoch 10090, loss: 10.166\n",
      "Epoch 10100, loss: 9.203\n",
      "Epoch 10110, loss: 9.598\n",
      "Epoch 10120, loss: 7.932\n",
      "Epoch 10130, loss: 10.603\n",
      "Epoch 10140, loss: 7.963\n",
      "Epoch 10150, loss: 8.084\n",
      "Epoch 10160, loss: 8.349\n",
      "Epoch 10170, loss: 11.421\n",
      "Epoch 10180, loss: 9.114\n",
      "Epoch 10190, loss: 7.363\n",
      "Epoch 10200, loss: 8.696\n",
      "Epoch 10210, loss: 8.755\n",
      "Epoch 10220, loss: 8.622\n",
      "Epoch 10230, loss: 10.361\n",
      "Epoch 10240, loss: 7.932\n",
      "Epoch 10250, loss: 9.573\n",
      "Epoch 10260, loss: 9.640\n",
      "Epoch 10270, loss: 8.174\n",
      "Epoch 10280, loss: 9.968\n",
      "Epoch 10290, loss: 8.349\n",
      "Epoch 10300, loss: 8.689\n",
      "Epoch 10310, loss: 9.815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10320, loss: 8.107\n",
      "Epoch 10330, loss: 9.223\n",
      "Epoch 10340, loss: 8.633\n",
      "Epoch 10350, loss: 8.731\n",
      "Epoch 10360, loss: 9.356\n",
      "Epoch 10370, loss: 7.890\n",
      "Epoch 10380, loss: 12.013\n",
      "Epoch 10390, loss: 8.349\n",
      "Epoch 10400, loss: 11.554\n",
      "Epoch 10410, loss: 7.012\n",
      "Epoch 10420, loss: 8.458\n",
      "Epoch 10430, loss: 9.114\n",
      "Epoch 10440, loss: 8.853\n",
      "Epoch 10450, loss: 8.513\n",
      "Epoch 10460, loss: 7.055\n",
      "Epoch 10470, loss: 7.998\n",
      "Epoch 10480, loss: 11.270\n",
      "Epoch 10490, loss: 7.363\n",
      "Epoch 10500, loss: 9.247\n",
      "Epoch 10510, loss: 8.938\n",
      "Epoch 10520, loss: 9.440\n",
      "Epoch 10530, loss: 8.271\n",
      "Epoch 10540, loss: 9.815\n",
      "Epoch 10550, loss: 11.312\n",
      "Epoch 10560, loss: 9.598\n",
      "Epoch 10570, loss: 8.041\n",
      "Epoch 10580, loss: 7.648\n",
      "Epoch 10590, loss: 10.887\n",
      "Epoch 10600, loss: 7.998\n",
      "Epoch 10610, loss: 7.678\n",
      "Epoch 10620, loss: 10.294\n",
      "Epoch 10630, loss: 7.823\n",
      "Epoch 10640, loss: 7.998\n",
      "Epoch 10650, loss: 9.398\n",
      "Epoch 10660, loss: 7.823\n",
      "Epoch 10670, loss: 11.554\n",
      "Epoch 10680, loss: 8.349\n",
      "Epoch 10690, loss: 8.513\n",
      "Epoch 10700, loss: 10.252\n",
      "Epoch 10710, loss: 12.013\n",
      "Epoch 10720, loss: 8.240\n",
      "Epoch 10730, loss: 8.786\n",
      "Epoch 10740, loss: 10.119\n",
      "Epoch 10750, loss: 11.094\n",
      "Epoch 10760, loss: 8.349\n",
      "Epoch 10770, loss: 8.271\n",
      "Epoch 10780, loss: 11.445\n",
      "Epoch 10790, loss: 11.904\n",
      "Epoch 10800, loss: 9.573\n",
      "Epoch 10810, loss: 7.963\n",
      "Epoch 10820, loss: 10.033\n",
      "Epoch 10830, loss: 8.326\n",
      "Epoch 10840, loss: 10.887\n",
      "Epoch 10850, loss: 8.677\n",
      "Epoch 10860, loss: 10.033\n",
      "Epoch 10870, loss: 9.180\n",
      "Epoch 10880, loss: 7.648\n",
      "Epoch 10890, loss: 9.815\n",
      "Epoch 10900, loss: 7.854\n",
      "Epoch 10910, loss: 10.294\n",
      "Epoch 10920, loss: 10.677\n",
      "Epoch 10930, loss: 7.998\n",
      "Epoch 10940, loss: 8.326\n",
      "Epoch 10950, loss: 9.724\n",
      "Epoch 10960, loss: 9.247\n",
      "Epoch 10970, loss: 9.331\n",
      "Epoch 10980, loss: 9.465\n",
      "Epoch 10990, loss: 10.057\n",
      "Epoch 11000, loss: 8.174\n",
      "Epoch 11010, loss: 9.114\n",
      "Epoch 11020, loss: 9.203\n",
      "Epoch 11030, loss: 9.815\n",
      "Epoch 11040, loss: 8.458\n",
      "Epoch 11050, loss: 8.349\n",
      "Epoch 11060, loss: 9.640\n",
      "Epoch 11070, loss: 9.991\n",
      "Epoch 11080, loss: 9.289\n",
      "Epoch 11090, loss: 7.363\n",
      "Epoch 11100, loss: 8.633\n",
      "Epoch 11110, loss: 9.991\n",
      "Epoch 11120, loss: 8.633\n",
      "Epoch 11130, loss: 10.210\n",
      "Epoch 11140, loss: 7.581\n",
      "Epoch 11150, loss: 8.853\n",
      "Epoch 11160, loss: 9.924\n",
      "Epoch 11170, loss: 8.174\n",
      "Epoch 11180, loss: 8.054\n",
      "Epoch 11190, loss: 9.422\n",
      "Epoch 11200, loss: 9.309\n",
      "Epoch 11210, loss: 10.077\n",
      "Epoch 11220, loss: 12.013\n",
      "Epoch 11230, loss: 7.890\n",
      "Epoch 11240, loss: 7.714\n",
      "Epoch 11250, loss: 9.356\n",
      "Epoch 11260, loss: 9.773\n",
      "Epoch 11270, loss: 8.240\n",
      "Epoch 11280, loss: 7.570\n",
      "Epoch 11290, loss: 8.696\n",
      "Epoch 11300, loss: 9.422\n",
      "Epoch 11310, loss: 7.998\n",
      "Epoch 11320, loss: 7.406\n",
      "Epoch 11330, loss: 9.640\n",
      "Epoch 11340, loss: 11.620\n",
      "Epoch 11350, loss: 7.648\n",
      "Epoch 11360, loss: 11.270\n",
      "Epoch 11370, loss: 11.094\n",
      "Epoch 11380, loss: 9.991\n",
      "Epoch 11390, loss: 10.736\n",
      "Epoch 11400, loss: 11.203\n",
      "Epoch 11410, loss: 9.114\n",
      "Epoch 11420, loss: 9.659\n",
      "Epoch 11430, loss: 7.890\n",
      "Epoch 11440, loss: 9.356\n",
      "Epoch 11450, loss: 9.815\n",
      "Epoch 11460, loss: 8.174\n",
      "Epoch 11470, loss: 9.640\n",
      "Epoch 11480, loss: 8.041\n",
      "Epoch 11490, loss: 9.422\n",
      "Epoch 11500, loss: 10.208\n",
      "Epoch 11510, loss: 8.568\n",
      "Epoch 11520, loss: 9.331\n",
      "Epoch 11530, loss: 9.356\n",
      "Epoch 11540, loss: 8.283\n",
      "Epoch 11550, loss: 10.494\n",
      "Epoch 11560, loss: 11.270\n",
      "Epoch 11570, loss: 11.554\n",
      "Epoch 11580, loss: 9.203\n",
      "Epoch 11590, loss: 7.539\n",
      "Epoch 11600, loss: 11.554\n",
      "Epoch 11610, loss: 7.363\n",
      "Epoch 11620, loss: 9.659\n",
      "Epoch 11630, loss: 7.714\n",
      "Epoch 11640, loss: 11.270\n",
      "Epoch 11650, loss: 11.729\n",
      "Epoch 11660, loss: 10.603\n",
      "Epoch 11670, loss: 8.151\n",
      "Epoch 11680, loss: 7.012\n",
      "Epoch 11690, loss: 11.028\n",
      "Epoch 11700, loss: 9.422\n",
      "Epoch 11710, loss: 8.719\n",
      "Epoch 11720, loss: 9.991\n",
      "Epoch 11730, loss: 8.174\n",
      "Epoch 11740, loss: 8.633\n",
      "Epoch 11750, loss: 9.507\n",
      "Epoch 11760, loss: 7.932\n",
      "Epoch 11770, loss: 10.844\n",
      "Epoch 11780, loss: 10.119\n",
      "Epoch 11790, loss: 9.882\n",
      "Epoch 11800, loss: 9.289\n",
      "Epoch 11810, loss: 9.598\n",
      "Epoch 11820, loss: 8.174\n",
      "Epoch 11830, loss: 7.363\n",
      "Epoch 11840, loss: 9.924\n",
      "Epoch 11850, loss: 8.525\n",
      "Epoch 11860, loss: 10.099\n",
      "Epoch 11870, loss: 8.271\n",
      "Epoch 11880, loss: 10.143\n",
      "Epoch 11890, loss: 10.427\n",
      "Epoch 11900, loss: 10.294\n",
      "Epoch 11910, loss: 8.349\n",
      "Epoch 11920, loss: 8.853\n",
      "Epoch 11930, loss: 9.203\n",
      "Epoch 11940, loss: 10.494\n",
      "Epoch 11950, loss: 10.101\n",
      "Epoch 11960, loss: 8.633\n",
      "Epoch 11970, loss: 8.065\n",
      "Epoch 11980, loss: 8.205\n",
      "Epoch 11990, loss: 9.090\n",
      "Epoch 12000, loss: 8.872\n",
      "Epoch 12010, loss: 7.297\n",
      "Epoch 12020, loss: 8.525\n",
      "Epoch 12030, loss: 7.890\n",
      "Epoch 12040, loss: 7.363\n",
      "Epoch 12050, loss: 11.904\n",
      "Epoch 12060, loss: 7.363\n",
      "Epoch 12070, loss: 9.924\n",
      "Epoch 12080, loss: 9.005\n",
      "Epoch 12090, loss: 8.283\n",
      "Epoch 12100, loss: 9.598\n",
      "Epoch 12110, loss: 8.973\n",
      "Epoch 12120, loss: 11.421\n",
      "Epoch 12130, loss: 10.778\n",
      "Epoch 12140, loss: 7.648\n",
      "Epoch 12150, loss: 7.909\n",
      "Epoch 12160, loss: 9.924\n",
      "Epoch 12170, loss: 7.998\n",
      "Epoch 12180, loss: 8.084\n",
      "Epoch 12190, loss: 8.906\n",
      "Epoch 12200, loss: 10.143\n",
      "Epoch 12210, loss: 7.932\n",
      "Epoch 12220, loss: 7.812\n",
      "Epoch 12230, loss: 8.054\n",
      "Epoch 12240, loss: 10.252\n",
      "Epoch 12250, loss: 10.887\n",
      "Epoch 12260, loss: 10.961\n",
      "Epoch 12270, loss: 7.998\n",
      "Epoch 12280, loss: 7.998\n",
      "Epoch 12290, loss: 9.179\n",
      "Epoch 12300, loss: 8.786\n",
      "Epoch 12310, loss: 9.465\n",
      "Epoch 12320, loss: 9.156\n",
      "Epoch 12330, loss: 8.906\n",
      "Epoch 12340, loss: 10.711\n",
      "Epoch 12350, loss: 8.853\n",
      "Epoch 12360, loss: 7.188\n",
      "Epoch 12370, loss: 8.162\n",
      "Epoch 12380, loss: 9.223\n",
      "Epoch 12390, loss: 8.696\n",
      "Epoch 12400, loss: 9.598\n",
      "Epoch 12410, loss: 11.378\n",
      "Epoch 12420, loss: 9.749\n",
      "Epoch 12430, loss: 7.823\n",
      "Epoch 12440, loss: 10.341\n",
      "Epoch 12450, loss: 9.507\n",
      "Epoch 12460, loss: 9.991\n",
      "Epoch 12470, loss: 12.013\n",
      "Epoch 12480, loss: 10.778\n",
      "Epoch 12490, loss: 8.630\n",
      "Epoch 12500, loss: 10.099\n",
      "Epoch 12510, loss: 9.924\n",
      "Epoch 12520, loss: 8.513\n",
      "Epoch 12530, loss: 9.265\n",
      "Epoch 12540, loss: 8.326\n",
      "Epoch 12550, loss: 7.963\n",
      "Epoch 12560, loss: 9.465\n",
      "Epoch 12570, loss: 12.013\n",
      "Epoch 12580, loss: 9.815\n",
      "Epoch 12590, loss: 9.090\n",
      "Epoch 12600, loss: 11.203\n",
      "Epoch 12610, loss: 7.648\n",
      "Epoch 12620, loss: 8.404\n",
      "Epoch 12630, loss: 8.174\n",
      "Epoch 12640, loss: 8.689\n",
      "Epoch 12650, loss: 9.573\n",
      "Epoch 12660, loss: 11.270\n",
      "Epoch 12670, loss: 10.786\n",
      "Epoch 12680, loss: 10.077\n",
      "Epoch 12690, loss: 7.998\n",
      "Epoch 12700, loss: 10.275\n",
      "Epoch 12710, loss: 9.422\n",
      "Epoch 12720, loss: 8.731\n",
      "Epoch 12730, loss: 7.998\n",
      "Epoch 12740, loss: 7.297\n",
      "Epoch 12750, loss: 8.853\n",
      "Epoch 12760, loss: 11.554\n",
      "Epoch 12770, loss: 9.180\n",
      "Epoch 12780, loss: 9.507\n",
      "Epoch 12790, loss: 10.341\n",
      "Epoch 12800, loss: 11.838\n",
      "Epoch 12810, loss: 10.294\n",
      "Epoch 12820, loss: 7.823\n",
      "Epoch 12830, loss: 7.297\n",
      "Epoch 12840, loss: 9.507\n",
      "Epoch 12850, loss: 9.706\n",
      "Epoch 12860, loss: 9.114\n",
      "Epoch 12870, loss: 7.920\n",
      "Epoch 12880, loss: 7.823\n",
      "Epoch 12890, loss: 11.620\n",
      "Epoch 12900, loss: 7.363\n",
      "Epoch 12910, loss: 8.633\n",
      "Epoch 12920, loss: 11.554\n",
      "Epoch 12930, loss: 11.729\n",
      "Epoch 12940, loss: 10.077\n",
      "Epoch 12950, loss: 8.622\n",
      "Epoch 12960, loss: 10.450\n",
      "Epoch 12970, loss: 9.507\n",
      "Epoch 12980, loss: 9.749\n",
      "Epoch 12990, loss: 9.924\n",
      "Epoch 13000, loss: 9.573\n",
      "Epoch 13010, loss: 8.447\n",
      "Epoch 13020, loss: 9.156\n",
      "Epoch 13030, loss: 9.882\n",
      "Epoch 13040, loss: 10.294\n",
      "Epoch 13050, loss: 8.404\n",
      "Epoch 13060, loss: 7.539\n",
      "Epoch 13070, loss: 8.786\n",
      "Epoch 13080, loss: 7.998\n",
      "Epoch 13090, loss: 10.450\n",
      "Epoch 13100, loss: 9.247\n",
      "Epoch 13110, loss: 8.271\n",
      "Epoch 13120, loss: 8.205\n",
      "Epoch 13130, loss: 7.678\n",
      "Epoch 13140, loss: 11.729\n",
      "Epoch 13150, loss: 8.458\n",
      "Epoch 13160, loss: 8.404\n",
      "Epoch 13170, loss: 11.554\n",
      "Epoch 13180, loss: 7.909\n",
      "Epoch 13190, loss: 7.932\n",
      "Epoch 13200, loss: 11.237\n",
      "Epoch 13210, loss: 8.689\n",
      "Epoch 13220, loss: 10.166\n",
      "Epoch 13230, loss: 9.465\n",
      "Epoch 13240, loss: 7.890\n",
      "Epoch 13250, loss: 7.812\n",
      "Epoch 13260, loss: 9.598\n",
      "Epoch 13270, loss: 8.630\n",
      "Epoch 13280, loss: 9.028\n",
      "Epoch 13290, loss: 11.904\n",
      "Epoch 13300, loss: 8.404\n",
      "Epoch 13310, loss: 9.640\n",
      "Epoch 13320, loss: 8.872\n",
      "Epoch 13330, loss: 8.513\n",
      "Epoch 13340, loss: 8.700\n",
      "Epoch 13350, loss: 8.513\n",
      "Epoch 13360, loss: 8.041\n",
      "Epoch 13370, loss: 8.240\n",
      "Epoch 13380, loss: 9.223\n",
      "Epoch 13390, loss: 7.012\n",
      "Epoch 13400, loss: 9.991\n",
      "Epoch 13410, loss: 9.422\n",
      "Epoch 13420, loss: 7.890\n",
      "Epoch 13430, loss: 7.297\n",
      "Epoch 13440, loss: 9.507\n",
      "Epoch 13450, loss: 8.174\n",
      "Epoch 13460, loss: 9.507\n",
      "Epoch 13470, loss: 9.640\n",
      "Epoch 13480, loss: 9.440\n",
      "Epoch 13490, loss: 11.028\n",
      "Epoch 13500, loss: 8.677\n",
      "Epoch 13510, loss: 10.033\n",
      "Epoch 13520, loss: 8.938\n",
      "Epoch 13530, loss: 8.919\n",
      "Epoch 13540, loss: 9.070\n",
      "Epoch 13550, loss: 7.297\n",
      "Epoch 13560, loss: 7.539\n",
      "Epoch 13570, loss: 7.890\n",
      "Epoch 13580, loss: 8.525\n",
      "Epoch 13590, loss: 9.507\n",
      "Epoch 13600, loss: 8.853\n",
      "Epoch 13610, loss: 8.174\n",
      "Epoch 13620, loss: 9.773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13630, loss: 7.998\n",
      "Epoch 13640, loss: 8.525\n",
      "Epoch 13650, loss: 8.174\n",
      "Epoch 13660, loss: 9.682\n",
      "Epoch 13670, loss: 7.812\n",
      "Epoch 13680, loss: 8.283\n",
      "Epoch 13690, loss: 7.363\n",
      "Epoch 13700, loss: 8.458\n",
      "Epoch 13710, loss: 9.398\n",
      "Epoch 13720, loss: 9.331\n",
      "Epoch 13730, loss: 10.210\n",
      "Epoch 13740, loss: 9.070\n",
      "Epoch 13750, loss: 7.823\n",
      "Epoch 13760, loss: 11.554\n",
      "Epoch 13770, loss: 7.998\n",
      "Epoch 13780, loss: 8.447\n",
      "Epoch 13790, loss: 9.598\n",
      "Epoch 13800, loss: 9.616\n",
      "Epoch 13810, loss: 9.247\n",
      "Epoch 13820, loss: 8.786\n",
      "Epoch 13830, loss: 8.696\n",
      "Epoch 13840, loss: 8.786\n",
      "Epoch 13850, loss: 12.363\n",
      "Epoch 13860, loss: 7.920\n",
      "Epoch 13870, loss: 8.696\n",
      "Epoch 13880, loss: 10.057\n",
      "Epoch 13890, loss: 8.544\n",
      "Epoch 13900, loss: 10.185\n",
      "Epoch 13910, loss: 7.920\n",
      "Epoch 13920, loss: 12.363\n",
      "Epoch 13930, loss: 7.998\n",
      "Epoch 13940, loss: 11.312\n",
      "Epoch 13950, loss: 7.932\n",
      "Epoch 13960, loss: 8.283\n",
      "Epoch 13970, loss: 9.968\n",
      "Epoch 13980, loss: 9.598\n",
      "Epoch 13990, loss: 9.815\n",
      "Epoch 14000, loss: 9.356\n",
      "Epoch 14010, loss: 11.203\n",
      "Epoch 14020, loss: 8.349\n",
      "Epoch 14030, loss: 9.179\n",
      "Epoch 14040, loss: 9.507\n",
      "Epoch 14050, loss: 10.208\n",
      "Epoch 14060, loss: 8.174\n",
      "Epoch 14070, loss: 9.616\n",
      "Epoch 14080, loss: 11.094\n",
      "Epoch 14090, loss: 9.991\n",
      "Epoch 14100, loss: 11.445\n",
      "Epoch 14110, loss: 9.507\n",
      "Epoch 14120, loss: 11.270\n",
      "Epoch 14130, loss: 7.998\n",
      "Epoch 14140, loss: 7.678\n",
      "Epoch 14150, loss: 8.513\n",
      "Epoch 14160, loss: 8.611\n",
      "Epoch 14170, loss: 9.356\n",
      "Epoch 14180, loss: 11.729\n",
      "Epoch 14190, loss: 9.398\n",
      "Epoch 14200, loss: 8.338\n",
      "Epoch 14210, loss: 12.013\n",
      "Epoch 14220, loss: 10.057\n",
      "Epoch 14230, loss: 9.815\n",
      "Epoch 14240, loss: 8.174\n",
      "Epoch 14250, loss: 9.773\n",
      "Epoch 14260, loss: 7.890\n",
      "Epoch 14270, loss: 8.786\n",
      "Epoch 14280, loss: 8.404\n",
      "Epoch 14290, loss: 7.823\n",
      "Epoch 14300, loss: 9.356\n",
      "Epoch 14310, loss: 11.554\n",
      "Epoch 14320, loss: 11.904\n",
      "Epoch 14330, loss: 9.180\n",
      "Epoch 14340, loss: 9.507\n",
      "Epoch 14350, loss: 7.363\n",
      "Epoch 14360, loss: 8.719\n",
      "Epoch 14370, loss: 10.033\n",
      "Epoch 14380, loss: 10.099\n",
      "Epoch 14390, loss: 7.854\n",
      "Epoch 14400, loss: 8.981\n",
      "Epoch 14410, loss: 9.356\n",
      "Epoch 14420, loss: 10.294\n",
      "Epoch 14430, loss: 8.271\n",
      "Epoch 14440, loss: 10.294\n",
      "Epoch 14450, loss: 7.890\n",
      "Epoch 14460, loss: 8.404\n",
      "Epoch 14470, loss: 8.513\n",
      "Epoch 14480, loss: 7.890\n",
      "Epoch 14490, loss: 8.271\n",
      "Epoch 14500, loss: 9.398\n",
      "Epoch 14510, loss: 8.458\n",
      "Epoch 14520, loss: 7.823\n",
      "Epoch 14530, loss: 8.065\n",
      "Epoch 14540, loss: 9.598\n",
      "Epoch 14550, loss: 8.174\n",
      "Epoch 14560, loss: 8.633\n",
      "Epoch 14570, loss: 8.349\n",
      "Epoch 14580, loss: 11.904\n",
      "Epoch 14590, loss: 7.690\n",
      "Epoch 14600, loss: 10.210\n",
      "Epoch 14610, loss: 9.991\n",
      "Epoch 14620, loss: 8.458\n",
      "Epoch 14630, loss: 7.998\n",
      "Epoch 14640, loss: 7.932\n",
      "Epoch 14650, loss: 9.331\n",
      "Epoch 14660, loss: 7.581\n",
      "Epoch 14670, loss: 11.270\n",
      "Epoch 14680, loss: 7.812\n",
      "Epoch 14690, loss: 8.349\n",
      "Epoch 14700, loss: 11.270\n",
      "Epoch 14710, loss: 9.706\n",
      "Epoch 14720, loss: 9.573\n",
      "Epoch 14730, loss: 9.356\n",
      "Epoch 14740, loss: 7.932\n",
      "Epoch 14750, loss: 9.815\n",
      "Epoch 14760, loss: 8.174\n",
      "Epoch 14770, loss: 9.507\n",
      "Epoch 14780, loss: 9.180\n",
      "Epoch 14790, loss: 7.932\n",
      "Epoch 14800, loss: 8.544\n",
      "Epoch 14810, loss: 9.706\n",
      "Epoch 14820, loss: 10.778\n",
      "Epoch 14830, loss: 9.749\n",
      "Epoch 14840, loss: 11.554\n",
      "Epoch 14850, loss: 8.700\n",
      "Epoch 14860, loss: 8.622\n",
      "Epoch 14870, loss: 9.356\n",
      "Epoch 14880, loss: 8.174\n",
      "Epoch 14890, loss: 7.932\n",
      "Epoch 14900, loss: 10.101\n",
      "Epoch 14910, loss: 11.554\n",
      "Epoch 14920, loss: 8.938\n",
      "Epoch 14930, loss: 10.427\n",
      "Epoch 14940, loss: 8.458\n",
      "Epoch 14950, loss: 8.677\n",
      "Epoch 14960, loss: 8.689\n",
      "Epoch 14970, loss: 7.539\n",
      "Epoch 14980, loss: 9.882\n",
      "Epoch 14990, loss: 10.101\n",
      "Epoch 15000, loss: 9.901\n",
      "Epoch 15010, loss: 8.349\n",
      "Epoch 15020, loss: 9.550\n",
      "Epoch 15030, loss: 10.494\n",
      "Epoch 15040, loss: 7.121\n",
      "Epoch 15050, loss: 7.188\n",
      "Epoch 15060, loss: 8.205\n",
      "Epoch 15070, loss: 9.640\n",
      "Epoch 15080, loss: 8.458\n",
      "Epoch 15090, loss: 8.054\n",
      "Epoch 15100, loss: 9.991\n",
      "Epoch 15110, loss: 11.904\n",
      "Epoch 15120, loss: 9.265\n",
      "Epoch 15130, loss: 7.363\n",
      "Epoch 15140, loss: 9.331\n",
      "Epoch 15150, loss: 8.404\n",
      "Epoch 15160, loss: 7.932\n",
      "Epoch 15170, loss: 9.223\n",
      "Epoch 15180, loss: 11.203\n",
      "Epoch 15190, loss: 9.815\n",
      "Epoch 15200, loss: 10.033\n",
      "Epoch 15210, loss: 7.823\n",
      "Epoch 15220, loss: 8.447\n",
      "Epoch 15230, loss: 9.398\n",
      "Epoch 15240, loss: 9.815\n",
      "Epoch 15250, loss: 9.465\n",
      "Epoch 15260, loss: 9.991\n",
      "Epoch 15270, loss: 11.554\n",
      "Epoch 15280, loss: 9.398\n",
      "Epoch 15290, loss: 7.998\n",
      "Epoch 15300, loss: 9.507\n",
      "Epoch 15310, loss: 10.210\n",
      "Epoch 15320, loss: 9.882\n",
      "Epoch 15330, loss: 9.070\n",
      "Epoch 15340, loss: 7.998\n",
      "Epoch 15350, loss: 9.598\n",
      "Epoch 15360, loss: 7.756\n",
      "Epoch 15370, loss: 7.297\n",
      "Epoch 15380, loss: 9.663\n",
      "Epoch 15390, loss: 10.033\n",
      "Epoch 15400, loss: 8.797\n",
      "Epoch 15410, loss: 10.341\n",
      "Epoch 15420, loss: 10.669\n",
      "Epoch 15430, loss: 8.696\n",
      "Epoch 15440, loss: 7.363\n",
      "Epoch 15450, loss: 11.203\n",
      "Epoch 15460, loss: 11.662\n",
      "Epoch 15470, loss: 9.379\n",
      "Epoch 15480, loss: 9.749\n",
      "Epoch 15490, loss: 11.445\n",
      "Epoch 15500, loss: 7.963\n",
      "Epoch 15510, loss: 8.240\n",
      "Epoch 15520, loss: 7.703\n",
      "Epoch 15530, loss: 7.012\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_115767/1148788206.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dist_matrix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PhD/projects/hyperbolic/hyper_dim_red/HyperDimRed/optimizers.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "from scipy.sparse.csgraph import dijkstra\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "from constants import *\n",
    "from methods import *\n",
    "from optimizers import * \n",
    "from utils.visulization import *\n",
    "from OdorDataset import OdorMonoDataset\n",
    "from utils.helpers import *\n",
    "latent_dim = 2\n",
    "lr = 0.1\n",
    "num_epochs = 100000\n",
    "normalize = False\n",
    "geodesic = False\n",
    "min_dist = 1.\n",
    "\n",
    "\n",
    "\n",
    "dataset_name='gslf'\n",
    "model_name = 'molformer'\n",
    "batch_size =10\n",
    "\n",
    "def set_seeds(seed):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed(seed)\n",
    "    # torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "def select_descriptors(dataset_name):\n",
    "    if dataset_name=='sagar':\n",
    "        return sagar_descriptors\n",
    "    elif dataset_name=='keller':\n",
    "        return keller_descriptors\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# set_seeds(2025)\n",
    "base_dir = '../../../T5 EVO/alignment_olfaction_datasets/curated_datasets/'\n",
    "input_embeddings = f'embeddings/{model_name}/{dataset_name}_{model_name}_embeddings_13_Apr17.csv'\n",
    "\n",
    "dataset = OdorMonoDataset(base_dir, None, transform=None, grand_avg=False, descriptors=select_descriptors(dataset_name))\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "\n",
    "    # del dataset\n",
    "\n",
    "\n",
    "def geo_distance(data):\n",
    "    #     truncated_matrix = torch.where(data_dist_matrix < min_dist, data_dist_matrix, torch.inf)\n",
    "    #     data_dist_matrix = dijkstra(truncated_matrix.detach().cpu().numpy())\n",
    "    #     data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "    #     data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix), data_dist_matrix)\n",
    "\n",
    "    data_nn_matrix = kneighbors_graph(data, 3, mode='distance', include_self=False)\n",
    "    data_nn_matrix = data_nn_matrix.toarray()\n",
    "    data_dist_matrix = dijkstra(data_nn_matrix)\n",
    "    data_dist_matrix = torch.FloatTensor(data_dist_matrix)\n",
    "    data_dist_matrix = torch.where(data_dist_matrix == torch.inf, 1000 * torch.ones_like(data_dist_matrix),data_dist_matrix)\n",
    "    return data_dist_matrix\n",
    "\n",
    "def nn_g(data):\n",
    "    data_nn_matrix = kneighbors_graph(data, 3, mode='distance', include_self=False)\n",
    "    data_nn_matrix = data_nn_matrix.toarray()\n",
    "    return data_nn_matrix\n",
    "\n",
    "#IsoMap-style geodesic distance for data\n",
    "\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.empty_cache()\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print('Using device: ', device)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#model = MDS(data.shape[0], latent_dim, Poincare)\n",
    "# print(len(dataset))\n",
    "# model = Isomap(len(dataset), latent_dim, Euclidean)\n",
    "# model = Contrastive(len(dataset), latent_dim, Euclidean)\n",
    "\n",
    "\n",
    "model = Contrastive(len(dataset), latent_dim, Poincare)\n",
    "\n",
    "#optimizer = StandardOptim(model, lr=lr)\n",
    "optimizer = PoincareOptim(model, lr=lr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for i in range(num_epochs):\n",
    "        total_loss=0\n",
    "\n",
    "        for idx, batch in data_loader:\n",
    "            if normalize:\n",
    "                model.normalize()\n",
    "            # if geodesic:\n",
    "            #     data_dist_matrix = geo_distance(batch)\n",
    "            # else:\n",
    "            #     data_dist_matrix = dist_matrix(batch, Euclidean)\n",
    "            data_nn_matrix = nn_g(batch)\n",
    "\n",
    "            #binary matrix\n",
    "            data_dist_matrix = (data_nn_matrix > 0).astype(int)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss_fun(data_dist_matrix,idx)\n",
    "            loss.backward()\n",
    "            optimizer.step(idx)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "            # print('norms', vector_norm(model.embeddings, dim=-1).mean().item(), vector_norm(model.embeddings, dim=-1).max().item())\n",
    "\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch {i}, loss: {total_loss/len(data_loader):.3f}')\n",
    "\n",
    "    scatterplot_2d(model.embeddings.detach().cpu().numpy(), labels=None, title='Poincare Embeddings')\n",
    "\n",
    "\n",
    "\n",
    "    size1 = (0.3, 0.28)\n",
    "    size2 = (0.6, 1)\n",
    "    size3 = (1, 0.35)\n",
    "\n",
    "    plt.rcParams[\"font.size\"] = 35\n",
    "    df_gslf_mols = prepare_goodscentleffignwell_mols(base_dir)\n",
    "    pom_frame(np.asarray(model.embeddings.detach().cpu().numpy().values.tolist()),\n",
    "              np.asarray(df_gslf_mols.y.values.tolist()), \"/kaggle/working/\", gs_lf_tasks, \"molformer\", size1, size2,\n",
    "              size3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2177bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def hasone(node_index, dim_index):\n",
    "    bin_i, bin_j = np.binary_repr(node_index), np.binary_repr(dim_index)\n",
    "    length = len(bin_j)\n",
    "    return (bin_i[:length] == bin_j) * 1\n",
    "\n",
    "def get_data(depth, dtype=np.float32):\n",
    "    n = 2**depth - 1\n",
    "    x = np.fromfunction(lambda i, j: np.vectorize(hasone)(i + 1, j + 1),\n",
    "                        (n, n), dtype=np.int32).astype(dtype)\n",
    "    return x\n",
    "\n",
    "#Load the data\n",
    "depth = 11\n",
    "binary_tree = get_data(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe79fc81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
