{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "# import wandb\n",
    "from SyntheticTreeDataset import *\n",
    "from OdorDataset import OdorMonoDataset\n",
    "from utils.helpers import *\n",
    "from methods import *\n",
    "from optimizers import *\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.visualization import *\n",
    "import uuid\n",
    "from utils.helpers import set_seeds\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "import scipy\n",
    "from torch.optim import Adam\n",
    "\n",
    "from distances import (\n",
    "    distance_matrix,\n",
    "    euclidean_distance,\n",
    "    poincare_distance,\n",
    "    knn_geodesic_distance_matrix,\n",
    "    knn_graph_weighted_adjacency_matrix,\n",
    "    # hamming_distance_matrix\n",
    ")\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "### If using Jupyter Notebook:###\n",
    "import sys\n",
    "if 'ipykernel_launcher' in sys.argv[0]:\n",
    "    sys.argv = sys.argv[:1]\n",
    "###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser('Hyperbolic Smell')\n",
    "    parser.add_argument('--data_type', type=str, default='labels' , choices={\"representation\",\"labels\"}) #label or batch\n",
    "    parser.add_argument('--representation_name', type=str, default='pom', choices={\"molformer\",\"pom\"})\n",
    "    parser.add_argument('--batch_size', type=int, default=195) #195\n",
    "    parser.add_argument('--num_epochs', type=int, default=5001) #100\n",
    "    # parser.add_argument('--min_dist', type=float, default=1.)\n",
    "    parser.add_argument('--latent_dim', type=int, default=2)\n",
    "    parser.add_argument('--lr', type=float, default=0.1)\n",
    "    # parser.add_argument('--lr', type=float, default=0.001)\n",
    "    parser.add_argument('--seed', type=int, default=1)\n",
    "    parser.add_argument('--base_dir', type=str,\n",
    "                        default='./data/')\n",
    "    parser.add_argument('--dataset_name', type=str, default='gslf' , choices={\"gslf\",\"ravia\",\"keller\",\"sagar\",\"sagarfmri\"})  # tree for synthetic, gslf for real\n",
    "    parser.add_argument('--normalize', type=bool, default=False) #* # only for Hyperbolic embeddings\n",
    "    parser.add_argument('--optimizer', type=str, default='standard', choices=['standard', 'poincare', 'Adam']) #*\n",
    "    parser.add_argument('--model_name', type=str, default='mds', choices=['isomap', 'mds', 'contrastive'])\n",
    "    parser.add_argument('--latent_dist_fun', type=str, default='euclidean', choices=['euclidean', 'poincare']) #*\n",
    "    parser.add_argument('--distr', type=str, default='gaussian', choices=['gaussian', 'hypergaussian']) #*\n",
    "    parser.add_argument('--distance_method', type=str, default='euclidean',\n",
    "                        choices=['geo', 'graph', 'hamming', 'euclidean','similarity']) #'euclidean' for sagar/keller, 'similarity' for ravia\n",
    "    parser.add_argument('--n_samples', type=int, default=4000)\n",
    "    parser.add_argument('--dim', type=int, default=768)\n",
    "    parser.add_argument('--depth', type=int, default=5)  # Changed from bool to int\n",
    "    parser.add_argument('--temperature', type=float, default=0.1)  # 0.1 #100\n",
    "    parser.add_argument('--n_neighbors', type=int, default=20) # 20 #10\n",
    "    parser.add_argument('--epsilon', type=float, default=10.0) #\n",
    "    parser.add_argument('--roi', type=str, default=None,choices=[\"OFC\", \"PirF\",\"PirT\",\"AMY\",None]) #\n",
    "    parser.add_argument('--subject', type=float, default=None,choices=[1,2,3,None]) #\n",
    "    parser.add_argument('--filter_dragon', type=bool, default=False) #for chemical data\n",
    "    # args = argparse.Namespace()\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        args.device = torch.device('cuda')\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        print('Using GPU')\n",
    "    else:\n",
    "        args.device = torch.device('cpu')\n",
    "        args.gpu_index = -1\n",
    "        print('Using CPU')\n",
    "\n",
    "    args.random_string = uuid.uuid4().hex\n",
    "    data_type = args.data_type\n",
    "    dataset_name = args.dataset_name\n",
    "    representation_name = args.representation_name\n",
    "    num_epochs = args.num_epochs\n",
    "    normalize = args.normalize\n",
    "    latent_dim = args.latent_dim\n",
    "    lr = args.lr\n",
    "    seed = args.seed\n",
    "    base_dir = args.base_dir\n",
    "    optimizer = args.optimizer\n",
    "    model_name = args.model_name\n",
    "    latent_dist_fun = args.latent_dist_fun\n",
    "    distr = args.distr\n",
    "    distance_method = args.distance_method\n",
    "    n_neighbors = args.n_neighbors\n",
    "    epsilon = args.epsilon\n",
    "    temperature = args.temperature\n",
    "    subject = args.subject\n",
    "    roi = args.roi\n",
    "    ### Overwrite the batchsize ###\n",
    "    depth = args.depth\n",
    "    # args.batch_size = 2 ** args.depth - 1  # to get full batch\n",
    "    batch_size = args.batch_size\n",
    "    filter_dragon = args.filter_dragon\n",
    "#    set_seeds(seed)\n",
    "\n",
    "    if distance_method == 'similarity' and dataset_name not in ['ravia']:\n",
    "        raise ValueError('Similarity distance method can only be used with Ravia dataset')\n",
    "\n",
    "    if dataset_name == 'tree':\n",
    "        embeddings, labels = get_tree_data(depth)\n",
    "        labels = torch.tensor(labels)\n",
    "        embeddings = torch.tensor(embeddings)\n",
    "        ## binary_tree is a dataset of binary sequences.\n",
    "        ## The root of the tree is the node 0: binary_tree[0]\n",
    "        ## groundtruth distance from node i to the root of the tree (i.e. shortest path distance from node i to the root): hamming_distance(binary_tree[0], binary_tree[i])\n",
    "        ## For visualizations, one can color a node by its groundtruth distance to the tree.\n",
    "    elif dataset_name == 'random':\n",
    "        #todo do we need this?\n",
    "        embeddings = torch.randn(n_samples, dim)\n",
    "    elif dataset_name in ['gslf', 'keller' , 'sagar']: ### If multiple subjects, to average among them put grand_avg=True. If individual subjects then put grand_avg=False and below use select_subjects function\n",
    "        input_embeddings = f'embeddings/{representation_name}/{dataset_name}_{representation_name}_embeddings_13_Apr17.csv'\n",
    "        embeddings, labels,subjects,CIDs = read_embeddings(base_dir, select_descriptors(dataset_name), input_embeddings,\n",
    "                                             grand_avg=True if (dataset_name == 'keller' or (dataset_name == 'saagar' and subject==None)) else False)\n",
    "        # embeddings, labels,subjects,CIDs = read_embeddings(base_dir, select_descriptors(dataset_name), input_embeddings,\n",
    "        #                                      grand_avg=True if dataset_name == 'keller' or dataset_name=='sagar' else False) #grand_avg averages among subjects so put false for analyzing each subject individually\n",
    "        # embeddings, labels,subjects,CIDs = read_embeddings(base_dir, select_descriptors(dataset_name), input_embeddings,\n",
    "        #                                      grand_avg=True if dataset_name=='sagar' else False) \n",
    " \n",
    "        if filter_dragon:\n",
    "            embeddings, labels, subjects, CIDs, embeddings_chemical=read_dragon_features(embeddings, labels, subjects, CIDs)\n",
    "            args.representation_name = 'chemical'\n",
    "            embeddings =  embeddings_chemical\n",
    "        #embeddings = 100000 * torch.randn(4983, 20)\n",
    "        \n",
    "        #To perform PCA or t-SNE on MolFormer or POM enbeddings:\n",
    "        # X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "        #          init='random', perplexity=300).fit_transform(embeddings)\n",
    "        #X_embedded = PCA(n_components=2).fit_transform(embeddings)\n",
    "\n",
    "        #Embed labels:\n",
    "        # X_embedded = TSNE(n_components=2, learning_rate='auto',\n",
    "        #          init='random', perplexity=1000).fit_transform(labels)\n",
    "        #X_embedded = PCA(n_components=2).fit_transform(labels)\n",
    "\n",
    "\n",
    "        # X_embedded = PCA(n_components=20).fit_transform(embeddings)\n",
    "        # embeddings = torch.tensor(X_embedded, dtype=torch.float32)  # Convert to a PyTorch tensor\n",
    "\n",
    "        # print('embeddings after PCA', embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
